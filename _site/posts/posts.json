[
  {
    "path": "posts/2022-07-21-technology-adoption/",
    "title": "Adding continent and country names with {countrycode}, and subsetting a data frame using sample()",
    "description": "Data wrangling and exploration to plot electricity production according to energy source and continent using the\n#TidyTuesday data set for week 29 of 2022\n  (19/7/2022): \"Technology Adoption\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2022-07-21",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSetup\r\nExploring tt$technology: selecting distinct values after filtering, and testing adding a “continent” variable\r\nWrangling tt$technology into two electricity production tibbles: fossil fuels and low-carbon sources\r\nPlotting distributions of electricity produced from fossil fuels and low-carbon sources\r\n\r\nIntroduction\r\nIn this post, the Technology Adoption data set is used to illustrate data exploration R and adding information using the {countrycode} package. During data exploration, the tt$technology data set is filtered to select for the “Energy” category, and the distinct values for “variable” and “label” are printed. A subset is then created to test adding full country names and corresponding continents based on 3 letter ISO codes in the data set using the countrycode() function. The full data set is then wrangled into two tibbles for fossil fuel and low-carbon electricity production: the distribution for each energy source is plotted according to the corresponding continent. The full source for this blog post is available on GitHub.\r\nSetup\r\nLoading the R libraries and data set.\r\n\r\n\r\n# Loading libraries\r\nlibrary(tidytuesdayR)\r\nlibrary(countrycode)\r\nlibrary(tidyverse)\r\nlibrary(ggthemes)\r\n\r\n# Loading data\r\ntt <- tt_load(\"2022-07-19\")\r\n\r\n\r\n    Downloading file 1 of 1: `technology.csv`\r\n\r\nExploring tt$technology: selecting distinct values after filtering, and testing adding a “continent” variable\r\n\r\n\r\n# Printing a summary of tt$technology\r\ntt$technology\r\n\r\n# A tibble: 491,636 × 7\r\n   variable label                      iso3c  year group categ…¹ value\r\n   <chr>    <chr>                      <chr> <dbl> <chr> <chr>   <dbl>\r\n 1 BCG      % children who received a… AFG    1982 Cons… Vaccin…    10\r\n 2 BCG      % children who received a… AFG    1983 Cons… Vaccin…    10\r\n 3 BCG      % children who received a… AFG    1984 Cons… Vaccin…    11\r\n 4 BCG      % children who received a… AFG    1985 Cons… Vaccin…    17\r\n 5 BCG      % children who received a… AFG    1986 Cons… Vaccin…    18\r\n 6 BCG      % children who received a… AFG    1987 Cons… Vaccin…    27\r\n 7 BCG      % children who received a… AFG    1988 Cons… Vaccin…    40\r\n 8 BCG      % children who received a… AFG    1989 Cons… Vaccin…    38\r\n 9 BCG      % children who received a… AFG    1990 Cons… Vaccin…    30\r\n10 BCG      % children who received a… AFG    1991 Cons… Vaccin…    21\r\n# … with 491,626 more rows, and abbreviated variable name ¹​category\r\n# ℹ Use `print(n = ...)` to see more rows\r\n\r\n# Printing the distinct \"variable\" and \"label\" pairs for the \"Energy\" category\r\n## This will be used as a reference to create the \"energy_type\" column/variable\r\ntt$technology %>% filter(category == \"Energy\") %>% select(variable, label) %>%\r\n  distinct()\r\n\r\n# A tibble: 11 × 2\r\n   variable              label                                        \r\n   <chr>                 <chr>                                        \r\n 1 elec_coal             Electricity from coal (TWH)                  \r\n 2 elec_cons             Electric power consumption (KWH)             \r\n 3 elec_gas              Electricity from gas (TWH)                   \r\n 4 elec_hydro            Electricity from hydro (TWH)                 \r\n 5 elec_nuc              Electricity from nuclear (TWH)               \r\n 6 elec_oil              Electricity from oil (TWH)                   \r\n 7 elec_renew_other      Electricity from other renewables (TWH)      \r\n 8 elec_solar            Electricity from solar (TWH)                 \r\n 9 elec_wind             Electricity from wind (TWH)                  \r\n10 elecprod              Gross output of electric energy (TWH)        \r\n11 electric_gen_capacity Electricity Generating Capacity, 1000 kilowa…\r\n\r\n# Setting a seed to make results reproducible\r\nset.seed(\"20220719\")\r\n# Using sample() to select six rows of tt$technology at random\r\nsample_rows <- sample(x = rownames(tt$technology), size = 6)\r\n# Creating a subset using the random rows\r\ntechnology_sample <- tt$technology[sample_rows, ]\r\n# Printing a summary of the randomly sampled subset\r\ntechnology_sample\r\n\r\n# A tibble: 6 × 7\r\n  variable        label               iso3c  year group categ…¹  value\r\n  <chr>           <chr>               <chr> <dbl> <chr> <chr>    <dbl>\r\n1 Pol3            % children who rec… PRY    1993 Cons… Vaccin… 6.6 e1\r\n2 pct_ag_ara_land % Arable land shar… LBR    1991 Non-… Agricu… 3.08e1\r\n3 fert_total      Aggregate kg of fe… CHE    1988 Prod… Agricu… 1.78e8\r\n4 railp           Thousands of passe… TUR    1948 Cons… Transp… 4.9 e1\r\n5 ag_land         Land agricultural … TUN    2013 Non-… Agricu… 9.94e3\r\n6 tv              Television sets     NIC    1981 Cons… Commun… 1.14e5\r\n# … with abbreviated variable name ¹​category\r\n\r\n# Adding continent and country name columns/variables to the sample subset,\r\n# using the countrycode::countrycode() function\r\ntechnology_sample <- technology_sample %>%\r\n  mutate(continent = countrycode(iso3c, origin = \"iso3c\",\r\n    destination = \"continent\"),\r\n    country = countrycode(iso3c, origin = \"iso3c\", destination = \"country.name\"))\r\n# Selecting the country ISO code, continent and country name of the sample\r\n# subset, to confirm that countrycode() worked as intended\r\ntechnology_sample %>% select(iso3c, continent, country)\r\n\r\n# A tibble: 6 × 3\r\n  iso3c continent country    \r\n  <chr> <chr>     <chr>      \r\n1 PRY   Americas  Paraguay   \r\n2 LBR   Africa    Liberia    \r\n3 CHE   Europe    Switzerland\r\n4 TUR   Asia      Turkey     \r\n5 TUN   Africa    Tunisia    \r\n6 NIC   Americas  Nicaragua  \r\n\r\nWrangling tt$technology into two electricity production tibbles: fossil fuels and low-carbon sources\r\n\r\n\r\n# Adding the corresponding continent for each country in tt$technology;\r\n# filtering to select for the \"Energy\" category; adding a more succinct\r\n# \"energy_type\" variable; and dropping rows with missing values\r\nenergy_tbl <- tt$technology %>%\r\n  mutate(continent = countrycode(iso3c, origin = \"iso3c\",\r\n    destination = \"continent\")) %>%\r\n  filter(category == \"Energy\") %>%\r\n  mutate(energy_type = fct_recode(variable,\r\n    \"Consumption\" = \"elec_cons\", \"Coal\" = \"elec_coal\", \"Gas\" = \"elec_gas\",\r\n    \"Hydro\" = \"elec_hydro\", \"Nuclear\" = \"elec_nuc\", \"Oil\" = \"elec_oil\",\r\n    \"Other renewables\" = \"elec_renew_other\", \"Solar\" = \"elec_solar\",\r\n    \"Wind\" = \"elec_wind\", \"Output\" = \"elecprod\",\r\n    \"Capacity\" = \"electric_gen_capacity\")) %>%\r\n  drop_na()\r\n\r\n# Printing a summary of energy_tbl\r\nenergy_tbl\r\n\r\n# A tibble: 66,300 × 9\r\n   variable  label     iso3c  year group categ…¹ value conti…² energ…³\r\n   <chr>     <chr>     <chr> <dbl> <chr> <chr>   <dbl> <chr>   <fct>  \r\n 1 elec_coal Electric… ABW    2000 Prod… Energy      0 Americ… Coal   \r\n 2 elec_coal Electric… ABW    2001 Prod… Energy      0 Americ… Coal   \r\n 3 elec_coal Electric… ABW    2002 Prod… Energy      0 Americ… Coal   \r\n 4 elec_coal Electric… ABW    2003 Prod… Energy      0 Americ… Coal   \r\n 5 elec_coal Electric… ABW    2004 Prod… Energy      0 Americ… Coal   \r\n 6 elec_coal Electric… ABW    2005 Prod… Energy      0 Americ… Coal   \r\n 7 elec_coal Electric… ABW    2006 Prod… Energy      0 Americ… Coal   \r\n 8 elec_coal Electric… ABW    2007 Prod… Energy      0 Americ… Coal   \r\n 9 elec_coal Electric… ABW    2008 Prod… Energy      0 Americ… Coal   \r\n10 elec_coal Electric… ABW    2009 Prod… Energy      0 Americ… Coal   \r\n# … with 66,290 more rows, and abbreviated variable names ¹​category,\r\n#   ²​continent, ³​energy_type\r\n# ℹ Use `print(n = ...)` to see more rows\r\n\r\n# Filtering energy_table for fossil fuel rows\r\nfossil_fuel_tbl <- energy_tbl %>%\r\n  filter(energy_type != \"Consumption\" & energy_type != \"Output\" \r\n    & energy_type != \"Capacity\") %>% \r\n  filter(energy_type == \"Coal\" | energy_type == \"Gas\" | energy_type == \"Oil\")\r\n\r\n# Printing a summary of the tibble\r\nfossil_fuel_tbl\r\n\r\n# A tibble: 13,914 × 9\r\n   variable  label     iso3c  year group categ…¹ value conti…² energ…³\r\n   <chr>     <chr>     <chr> <dbl> <chr> <chr>   <dbl> <chr>   <fct>  \r\n 1 elec_coal Electric… ABW    2000 Prod… Energy      0 Americ… Coal   \r\n 2 elec_coal Electric… ABW    2001 Prod… Energy      0 Americ… Coal   \r\n 3 elec_coal Electric… ABW    2002 Prod… Energy      0 Americ… Coal   \r\n 4 elec_coal Electric… ABW    2003 Prod… Energy      0 Americ… Coal   \r\n 5 elec_coal Electric… ABW    2004 Prod… Energy      0 Americ… Coal   \r\n 6 elec_coal Electric… ABW    2005 Prod… Energy      0 Americ… Coal   \r\n 7 elec_coal Electric… ABW    2006 Prod… Energy      0 Americ… Coal   \r\n 8 elec_coal Electric… ABW    2007 Prod… Energy      0 Americ… Coal   \r\n 9 elec_coal Electric… ABW    2008 Prod… Energy      0 Americ… Coal   \r\n10 elec_coal Electric… ABW    2009 Prod… Energy      0 Americ… Coal   \r\n# … with 13,904 more rows, and abbreviated variable names ¹​category,\r\n#   ²​continent, ³​energy_type\r\n# ℹ Use `print(n = ...)` to see more rows\r\n\r\n# Filtering energy_table for low-carbon energy source rows\r\nlow_carbon_tbl <- energy_tbl %>%\r\n  filter(energy_type != \"Consumption\" & energy_type != \"Output\" \r\n    & energy_type != \"Capacity\") %>% \r\n  filter(energy_type != \"Coal\" & energy_type != \"Gas\" & energy_type != \"Oil\")\r\n\r\n# Printing a summary of the tibble\r\nlow_carbon_tbl\r\n\r\n# A tibble: 26,890 × 9\r\n   variable   label    iso3c  year group categ…¹ value conti…² energ…³\r\n   <chr>      <chr>    <chr> <dbl> <chr> <chr>   <dbl> <chr>   <fct>  \r\n 1 elec_hydro Electri… ABW    2000 Prod… Energy      0 Americ… Hydro  \r\n 2 elec_hydro Electri… ABW    2001 Prod… Energy      0 Americ… Hydro  \r\n 3 elec_hydro Electri… ABW    2002 Prod… Energy      0 Americ… Hydro  \r\n 4 elec_hydro Electri… ABW    2003 Prod… Energy      0 Americ… Hydro  \r\n 5 elec_hydro Electri… ABW    2004 Prod… Energy      0 Americ… Hydro  \r\n 6 elec_hydro Electri… ABW    2005 Prod… Energy      0 Americ… Hydro  \r\n 7 elec_hydro Electri… ABW    2006 Prod… Energy      0 Americ… Hydro  \r\n 8 elec_hydro Electri… ABW    2007 Prod… Energy      0 Americ… Hydro  \r\n 9 elec_hydro Electri… ABW    2008 Prod… Energy      0 Americ… Hydro  \r\n10 elec_hydro Electri… ABW    2009 Prod… Energy      0 Americ… Hydro  \r\n# … with 26,880 more rows, and abbreviated variable names ¹​category,\r\n#   ²​continent, ³​energy_type\r\n# ℹ Use `print(n = ...)` to see more rows\r\n\r\nPlotting distributions of electricity produced from fossil fuels and low-carbon sources\r\n\r\n\r\n# Plotting distributions of electricity produced from fossil fuels\r\nfossil_fuel_tbl %>%\r\n  ggplot(aes(x = fct_reorder(energy_type, value), y = value, fill = energy_type)) +\r\n  geom_boxplot() +\r\n  theme_solarized() +\r\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\r\n    legend.position = \"none\") +\r\n  scale_colour_discrete() +\r\n  scale_y_log10() +\r\n  facet_wrap(~continent, scales = \"free\") +\r\n  labs(\r\n    title = \"Electricity generated from fossil fuels by continent\",\r\n    y = \"Output in log terawatt-hours: log10(TWh)\",\r\n    x = \"Source\")\r\n\r\n\r\n\r\nFigure 1: Box plots of electricity produced from fossil fuels, faceted by continent.\r\n\r\n\r\n\r\n\r\n\r\n# Plotting distributions of electricity produced from low-carbon sources\r\nlow_carbon_tbl %>%\r\n  ggplot(aes(x = fct_reorder(energy_type, value), y = value, fill = energy_type)) +\r\n  geom_boxplot() +\r\n  theme_solarized() +\r\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\r\n    legend.position = \"none\") +\r\n  scale_colour_discrete() +\r\n  scale_y_log10() +\r\n  facet_wrap(~continent, scales = \"free\") +\r\n  labs(\r\n    title = \"Electricity generated from low-carbon sources by continent\",\r\n    y = \"Output in log terawatt-hours: log10(TWh)\",\r\n    x = \"Source\")\r\n\r\n\r\n\r\nFigure 2: Box plots of electricity produced from low-carbon energy sources, faceted by continent.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-21-technology-adoption/technology-adoption_files/figure-html5/fig1-1.png",
    "last_modified": "2022-07-21T07:55:23+01:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 1152
  },
  {
    "path": "posts/2022-07-12-european-flights/",
    "title": "How to write a function in R and apply it to a data frame using map functions from {purrr}",
    "description": "Writing a function and applying it to a data frame using the\n#TidyTuesday data set for week 28 of 2022\n  (12/7/2022): \"European Flights\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2022-07-12",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSetup\r\nDefining a function to tidy flight types and applying it with purrr::map\r\nBinding the tidied flight type rows into a data frame with purrr::map_df\r\nPlotting the distribution of arrivals and departures across the top six airports\r\nSee also\r\n\r\nIntroduction\r\nIn this post, the European Flights data set is used to illustrate defining a function in R and applying it to a data frame using map functions from {purrr}. The full source for this blog post is available on GitHub.\r\nSetup\r\nLoading the R libraries and data set.\r\n\r\n\r\n# Loading libraries\r\nlibrary(tidytuesdayR)\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(ggthemes)\r\n\r\n# Loading data\r\ntt <- tt_load(\"2022-07-12\")\r\n\r\n\r\n    Downloading file 1 of 1: `flights.csv`\r\n\r\nDefining a function to tidy flight types and applying it with purrr::map\r\nIn this section, we want to tidy the different types of flight in the data set by increasing the number of rows and decreasing the number of columns. For a given airport on a given day, instead of having multiple columns/variables for arrivals, departures and total number of flights, we want to have one column describing the flight type (e.g. arrival or departure) and one column with the value of that flight type/number of flights. This will give the data set a tidy structure.\r\n\r\n\r\n# Printing a summary of the flights data frame\r\ntt$flights\r\n\r\n# A tibble: 688,099 × 14\r\n    YEAR MONTH_NUM MONTH_MON FLT_DATE            APT_ICAO APT_NAME    \r\n   <dbl> <chr>     <chr>     <dttm>              <chr>    <chr>       \r\n 1  2016 01        JAN       2016-01-01 00:00:00 EBAW     Antwerp     \r\n 2  2016 01        JAN       2016-01-01 00:00:00 EBBR     Brussels    \r\n 3  2016 01        JAN       2016-01-01 00:00:00 EBCI     Charleroi   \r\n 4  2016 01        JAN       2016-01-01 00:00:00 EBLG     Liège       \r\n 5  2016 01        JAN       2016-01-01 00:00:00 EBOS     Ostend-Brug…\r\n 6  2016 01        JAN       2016-01-01 00:00:00 EDDB     Berlin - Br…\r\n 7  2016 01        JAN       2016-01-01 00:00:00 EDDC     Dresden     \r\n 8  2016 01        JAN       2016-01-01 00:00:00 EDDE     Erfurt      \r\n 9  2016 01        JAN       2016-01-01 00:00:00 EDDF     Frankfurt   \r\n10  2016 01        JAN       2016-01-01 00:00:00 EDDG     Muenster-Os…\r\n# … with 688,089 more rows, and 8 more variables: STATE_NAME <chr>,\r\n#   FLT_DEP_1 <dbl>, FLT_ARR_1 <dbl>, FLT_TOT_1 <dbl>,\r\n#   FLT_DEP_IFR_2 <dbl>, FLT_ARR_IFR_2 <dbl>, FLT_TOT_IFR_2 <dbl>,\r\n#   `Pivot Label` <chr>\r\n\r\n# Printing a summary of the shape of the data frame\r\npaste(\"tt$flights has\", nrow(tt$flights), \"rows and\", ncol(tt$flights),\r\n  \"columns.\")\r\n\r\n[1] \"tt$flights has 688099 rows and 14 columns.\"\r\n\r\n# Defining a function to tidy the flights data set\r\ntidy_flights_per_airport <- function(input_flight_type){\r\n  tt$flights %>% \r\n    # Selecting columns, including the column with the name \"input_flight_type\"\r\n    ## \"all_of()\" is used for error handling: if a column with the name matching\r\n    ## \"input_flight_type\" is not available in tt$flights, the function will return an error\r\n    select(FLT_DATE, APT_NAME, all_of(input_flight_type)) %>% \r\n    # Adding a \"flight_type\" column, with \"input_flight_type\" as a string for each row\r\n    mutate(flight_type = as.character(input_flight_type)) %>% \r\n    # Renaming the input \"input_flight_type\" column to \"number_of_flights\"\r\n    rename(\"number_of_flights\" = input_flight_type)\r\n}\r\n\r\n# Selecting column names with flight types (arrivals, departures, total flights)\r\nflight_types <- colnames(tt$flights)[8:13]\r\n# Printing the flight types\r\nflight_types\r\n\r\n[1] \"FLT_DEP_1\"     \"FLT_ARR_1\"     \"FLT_TOT_1\"     \"FLT_DEP_IFR_2\"\r\n[5] \"FLT_ARR_IFR_2\" \"FLT_TOT_IFR_2\"\r\n\r\n# Applying the tidying function to the flight types vector using purrr::map()\r\ntidy_flights_list <- map(flight_types, tidy_flights_per_airport)\r\n\r\n\r\nBinding the tidied flight type rows into a data frame with purrr::map_df\r\nUsing the map function in the previous section returned a list of tidied flight types: the “tidy_flights_per_airport()” function was applied to each item in “flight_types” individually, and the resulting tidied flight type was added to “tidy_flights_list”. In this section, the “rbind()” function is applied to “tidy_flights_list” to create a single data frame with all of the tidied flight types.\r\n\r\n\r\n# Binding the tidy version of each flight type by row using purrr::map_df\r\ntidy_flights <- map_df(tidy_flights_list, rbind)\r\n\r\n# Printing a summary of the tidy flights data frame\r\ntidy_flights\r\n\r\n# A tibble: 4,128,594 × 4\r\n   FLT_DATE            APT_NAME           number_of_fligh… flight_type\r\n   <dttm>              <chr>                         <dbl> <chr>      \r\n 1 2016-01-01 00:00:00 Antwerp                           4 FLT_DEP_1  \r\n 2 2016-01-01 00:00:00 Brussels                        174 FLT_DEP_1  \r\n 3 2016-01-01 00:00:00 Charleroi                        45 FLT_DEP_1  \r\n 4 2016-01-01 00:00:00 Liège                             6 FLT_DEP_1  \r\n 5 2016-01-01 00:00:00 Ostend-Bruges                     7 FLT_DEP_1  \r\n 6 2016-01-01 00:00:00 Berlin - Brandenb…               98 FLT_DEP_1  \r\n 7 2016-01-01 00:00:00 Dresden                          18 FLT_DEP_1  \r\n 8 2016-01-01 00:00:00 Erfurt                            1 FLT_DEP_1  \r\n 9 2016-01-01 00:00:00 Frankfurt                       401 FLT_DEP_1  \r\n10 2016-01-01 00:00:00 Muenster-Osnabrue…                3 FLT_DEP_1  \r\n# … with 4,128,584 more rows\r\n\r\n# Printing a summary of the shape of the data frame\r\npaste(\"tidy_flights has\", nrow(tidy_flights), \"rows and\", ncol(tidy_flights),\r\n  \"columns.\")\r\n\r\n[1] \"tidy_flights has 4128594 rows and 4 columns.\"\r\n\r\nThe tidy_flights data frame is now in a tidy format.\r\nPlotting the distribution of arrivals and departures across the top six airports\r\n\r\n\r\n## Selecting the top 6 airports by total number of flights on the latest flight\r\n## date\r\ntop_airports <- tidy_flights %>%\r\n  filter(flight_type == \"FLT_TOT_1\") %>%\r\n  filter(FLT_DATE == max(FLT_DATE)) %>%\r\n  slice_max(order_by = number_of_flights, n = 6)\r\n\r\n# Changing \"flight_type\" to a factor with descriptive levels\r\ntidy_flights$flight_type <- as.factor(tidy_flights$flight_type)\r\nlevels(tidy_flights$flight_type) <- c(\"Arrivals\", \"Arrivals (Airport Operator)\",\r\n  \"Departures\", \"Departures (Airport Operator)\", \"Total\", \"Total (Airport Operator\")\r\n\r\n# Plotting the distribution of arrivals and departures for the top airports\r\ntidy_flights %>%\r\n  filter(APT_NAME %in% top_airports$APT_NAME) %>%\r\n  filter(flight_type %in% c(\"Arrivals\", \"Departures\")) %>%\r\n  ggplot(aes(x = APT_NAME, y = number_of_flights, colour = flight_type)) +\r\n  geom_boxplot() +\r\n  theme_solarized() +\r\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\r\n  scale_colour_discrete() +\r\n  labs(title = \"Distribution of daily arrivals and depatures across six airports\",\r\n    x = \"Airport\", y = \"Flights\", colour = \"Flight type\")\r\n\r\n\r\n\r\nFigure 1: Box plots of daily arrival and depature distribution across top six airports.\r\n\r\n\r\n\r\nSee also\r\nReshaping data using pivot functions\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-12-european-flights/european-flights_files/figure-html5/fig1-1.png",
    "last_modified": "2022-07-27T22:07:35+01:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 1152
  },
  {
    "path": "posts/2022-07-05-sf-rents/",
    "title": "Reshaping data frames using pivot functions from {tidyr} and tally from {dplyr}",
    "description": "Demonstrating methods for changing the shape of data frames (number of columns\nand rows) using the #TidyTuesday data set for week 27 of 2022\n  (5/7/2022): \"San Francisco Rentals\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2022-07-05",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSetup\r\nReshaping\r\na data frame by summarising variables\r\nReshaping\r\na data frame to make it wider with the {tidyr} function pivot\r\nwider\r\nReshaping\r\na data frame to make it more narrow with the {tidyr} function pivot\r\nlonger\r\nPlotting\r\npermit type counts per street using a tidy data frame of value\r\ncounts\r\nPlotting\r\nannual construction per San Francisco county using a data frame created\r\nwith pivot longer\r\n\r\nIntroduction\r\nIn this post, the San\r\nFrancisco Rentals data set is used to demonstrate data reshaping in\r\nR. This involves changing the number of columns and rows in a data frame\r\nto fit a given use case. A data frame is made more tall or narrow by\r\ndecreasing the number of columns, and wider by increasing the number of\r\ncolumns. The three reshaping methods covered in this article are:\r\nMaking a\r\ndata frame more narrow by summarising variables using group_by() and\r\ntally()\r\nMaking\r\na data frame wider with pivot_wider()\r\n“Lengthening”\r\na data frame with pivot_longer()\r\nData frames created with these methods were used to make two\r\nplots:\r\nCount\r\nof construction permits by type per street\r\nAnnual\r\nconstruction by type per San Francisco county\r\nSetup\r\nLoading the R libraries and\r\ndata\r\nset.\r\n\r\n\r\n# Loading libraries\r\nlibrary(tidytuesdayR)\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(ggthemes)\r\n\r\n# Loading data\r\ntt <- tt_load(\"2022-07-05\")\r\n\r\n\r\n    Downloading file 1 of 3: `rent.csv`\r\n    Downloading file 2 of 3: `sf_permits.csv`\r\n    Downloading file 3 of 3: `new_construction.csv`\r\n\r\nReshaping a\r\ndata frame by summarising variables\r\n\r\n\r\n# Printing a summary of the San Francisco (SF) permits data frame\r\ntt$sf_permits\r\n\r\n# A tibble: 86,103 × 44\r\n   permit_number permit_type permit_type_definiti… permit_creation_da…\r\n           <dbl>       <dbl> <chr>                 <dttm>             \r\n 1    2000010368           3 additions alteration… 2000-01-03 00:00:00\r\n 2    2000010353           6 demolitions           2000-01-03 00:00:00\r\n 3    2000010498           3 additions alteration… 2000-01-04 00:00:00\r\n 4    2000010484           3 additions alteration… 2000-01-04 00:00:00\r\n 5    2000010480           3 additions alteration… 2000-01-04 00:00:00\r\n 6    2000010475           3 additions alteration… 2000-01-04 00:00:00\r\n 7    2000010476           3 additions alteration… 2000-01-04 00:00:00\r\n 8    2000010474           3 additions alteration… 2000-01-04 00:00:00\r\n 9    2000010479           3 additions alteration… 2000-01-04 00:00:00\r\n10   20000104173           3 additions alteration… 2000-01-04 00:00:00\r\n# … with 86,093 more rows, and 40 more variables: block <chr>,\r\n#   lot <chr>, street_number <dbl>, street_number_suffix <chr>,\r\n#   street_name <chr>, street_suffix <chr>, unit <dbl>,\r\n#   unit_suffix <chr>, description <chr>, status <chr>,\r\n#   status_date <dttm>, filed_date <dttm>, issued_date <dttm>,\r\n#   completed_date <dttm>, first_construction_document_date <dttm>,\r\n#   structural_notification <chr>, …\r\n\r\n# Printing a summary of the shape of the data frame\r\npaste(\"tt$sf_permits has\", nrow(tt$sf_permits), \"rows and\", ncol(tt$sf_permits),\r\n  \"columns.\")\r\n\r\n[1] \"tt$sf_permits has 86103 rows and 44 columns.\"\r\n\r\n# Creating a tall/narrow data set of permits per street\r\npermits_per_street <- tt$sf_permits %>%\r\n  # Selecting variables/columns to keep\r\n  select(permit_type_definition, street_name, permit_number) %>%\r\n  # Grouping the permit numbers by type and street name for counting\r\n  group_by(permit_type_definition, street_name) %>%\r\n  # Counting/tallying the number of permits by type per street\r\n  tally()\r\n\r\n# Printing a summary of the permits per street data frame\r\npermits_per_street\r\n\r\n# A tibble: 3,053 × 3\r\n# Groups:   permit_type_definition [4]\r\n   permit_type_definition           street_name     n\r\n   <chr>                            <chr>       <int>\r\n 1 additions alterations or repairs 01st          196\r\n 2 additions alterations or repairs 02nd          763\r\n 3 additions alterations or repairs 03rd          778\r\n 4 additions alterations or repairs 04th          338\r\n 5 additions alterations or repairs 05th          223\r\n 6 additions alterations or repairs 06th          347\r\n 7 additions alterations or repairs 07th          199\r\n 8 additions alterations or repairs 08th          252\r\n 9 additions alterations or repairs 08th Ti         1\r\n10 additions alterations or repairs 09th          301\r\n# … with 3,043 more rows\r\n\r\n# Printing a summary of the shape of the data frame\r\npaste(\"permits_per_street has\", nrow(permits_per_street), \"rows and\",\r\n  ncol(permits_per_street), \"columns.\")\r\n\r\n[1] \"permits_per_street has 3053 rows and 3 columns.\"\r\n\r\nReshaping\r\na data frame to make it wider with the {tidyr} function pivot wider\r\n\r\n\r\n# Creating a wider copy of the permits per street data frame\r\npermits_per_street_wider <- permits_per_street %>%\r\n  # Pivoting the street names wider (creating a column for each street) and\r\n  # selecting the \"n\" variable for the values in this data frame\r\n  pivot_wider(names_from = street_name, values_from = n)\r\n\r\n# Printing the wider permits per street data frame\r\npermits_per_street_wider\r\n\r\n# A tibble: 4 × 1,588\r\n# Groups:   permit_type_definition [4]\r\n  permit_type_defini… `01st` `02nd` `03rd` `04th` `05th` `06th` `07th`\r\n  <chr>                <int>  <int>  <int>  <int>  <int>  <int>  <int>\r\n1 additions alterati…    196    763    778    338    223    347    199\r\n2 demolitions             16     17     72      8      7     17     24\r\n3 new construction         9      9     26      8      3      4     13\r\n4 new construction w…     NA      3     48      3      4      8      7\r\n# … with 1,580 more variables: `08th` <int>, `08th Ti` <int>,\r\n#   `09th` <int>, `10th` <int>, `11th` <int>, `12th` <int>,\r\n#   `13th` <int>, `13th Ti` <int>, `14th` <int>, `15th` <int>,\r\n#   `16th` <int>, `17th` <int>, `18th` <int>, `19th` <int>,\r\n#   `20th` <int>, `21st` <int>, `22nd` <int>, `23rd` <int>,\r\n#   `24th` <int>, `25th` <int>, `25th North` <int>, `26th` <int>,\r\n#   `27th` <int>, `28th` <int>, `29th` <int>, `2nd` <int>, …\r\n\r\n# Printing a summary of the shape of the data frame\r\npaste(\"permits_per_street_wider has\", nrow(permits_per_street_wider), \"rows and\",\r\n  ncol(permits_per_street_wider), \"columns.\")\r\n\r\n[1] \"permits_per_street_wider has 4 rows and 1588 columns.\"\r\n\r\nReshaping\r\na data frame to make it more narrow with the {tidyr} function pivot\r\nlonger\r\n\r\n\r\n# Printing a summary of the new construction data frame\r\ntt$new_construction\r\n\r\n# A tibble: 261 × 10\r\n   cartodb_id the_geom the_geom_webmerca… county  year totalproduction\r\n        <dbl> <lgl>    <lgl>              <chr>  <dbl>           <dbl>\r\n 1          1 NA       NA                 Alame…  1990            3601\r\n 2          2 NA       NA                 Alame…  1991             226\r\n 3          3 NA       NA                 Alame…  1992            2652\r\n 4          4 NA       NA                 Alame…  1993            3049\r\n 5          5 NA       NA                 Alame…  1994            2617\r\n 6          6 NA       NA                 Alame…  1995            3515\r\n 7          7 NA       NA                 Alame…  1996            3179\r\n 8          8 NA       NA                 Alame…  1997            4591\r\n 9          9 NA       NA                 Alame…  1998            6022\r\n10         10 NA       NA                 Alame…  1999            5601\r\n# … with 251 more rows, and 4 more variables: sfproduction <dbl>,\r\n#   mfproduction <dbl>, mhproduction <dbl>, source <chr>\r\n\r\n# Printing a summary of the shape of the data frame\r\npaste(\"tt$new_construction has\", nrow(tt$new_construction), \"rows and\",\r\n  ncol(tt$new_construction), \"columns.\")\r\n\r\n[1] \"tt$new_construction has 261 rows and 10 columns.\"\r\n\r\n# Creating a taller/more narrow subset of production type per county\r\nproduction_per_county <- tt$new_construction %>%\r\n  # Selecting variables/columns from tt$new_construction\r\n  select(county, year, totalproduction, sfproduction, mfproduction,mhproduction) %>%\r\n  # \"Lengthening\" the data frame by selecting columns to be pivoted to a longer format\r\n  pivot_longer(cols = c(totalproduction, sfproduction, mfproduction, mhproduction)) %>%\r\n  # Creating a copy of the \"name\" column to the more descriptive \"production_type\", as the\r\n  # pivoted columns all describe types of production, and removing the original \"name\"\r\n  # column\r\n  mutate(production_type = name, name = NULL) %>%\r\n  # Changing \"production_type\" from a character to a factor variable, with more\r\n  # descriptive factor levels\r\n  mutate(production_type = fct_recode(production_type,\r\n    \"Total\" = \"totalproduction\", \"Single family\" = \"sfproduction\",\r\n    \"Multi family\" = \"mfproduction\", \"Mobile home\" = \"mhproduction\"))\r\n\r\n# Printing a summary of the production per county data frame\r\nproduction_per_county\r\n\r\n# A tibble: 1,044 × 4\r\n   county          year value production_type\r\n   <chr>          <dbl> <dbl> <fct>          \r\n 1 Alameda County  1990  3601 Total          \r\n 2 Alameda County  1990  2166 Single family  \r\n 3 Alameda County  1990  1378 Multi family   \r\n 4 Alameda County  1990    57 Mobile home    \r\n 5 Alameda County  1991   226 Total          \r\n 6 Alameda County  1991  -236 Single family  \r\n 7 Alameda County  1991   395 Multi family   \r\n 8 Alameda County  1991    67 Mobile home    \r\n 9 Alameda County  1992  2652 Total          \r\n10 Alameda County  1992  2018 Single family  \r\n# … with 1,034 more rows\r\n\r\n# Printing a summary of the shape of the data frame\r\npaste(\"production_per_county has\", nrow(production_per_county), \"rows and\",\r\n  ncol(production_per_county), \"columns.\")\r\n\r\n[1] \"production_per_county has 1044 rows and 4 columns.\"\r\n\r\nPlotting\r\npermit type counts per street using a tidy data frame of value\r\ncounts\r\n\r\n\r\n# Plotting the top 20 streets with the total number of each permit category\r\npermits_per_street %>%\r\n  slice_max(order_by = n, n = 20) %>%\r\n  mutate(street_name = reorder_within(street_name, n, permit_type_definition)) %>%\r\n  ggplot(aes(x = n, y = street_name, fill = permit_type_definition)) +\r\n  geom_col(show.legend = FALSE) +\r\n  scale_y_reordered() +\r\n  theme_solarized_2() +\r\n  facet_wrap(~permit_type_definition, ncol = 2, scales = \"free\") +\r\n  labs(title = \"Count of construction permits by type per street\",\r\n    x = \"Tally\", y = \"Street name\")\r\n\r\n\r\n\r\nPlotting\r\nannual construction per San Francisco county using a data frame created\r\nwith pivot longer\r\n\r\n\r\n# Plotting the annual construction by type per San Francisco county\r\nproduction_per_county %>%\r\n  ggplot(aes(x = year, y = value,\r\n    colour = fct_reorder2(production_type, year, value))) +\r\n  geom_line() +\r\n  theme_clean() +\r\n  facet_wrap(~county, scales = \"free\") +\r\n  scale_colour_brewer(palette = \"Dark2\") +\r\n  scale_x_continuous(breaks =\r\n      seq(min(production_per_county$year), max(production_per_county$year), 8)) +\r\n  geom_vline(xintercept = 2008, linetype = 2, colour = \"red\", size = 0.4) +\r\n  labs(colour = \"Production type\", x = \"Year\", y = \"Units\",\r\n    title = \"Annual construction by type per San Francisco county\",\r\n    subtitle = \"Red vertical line marks 2008\")\r\n\r\n\r\n\r\nFigure 1: In San Francisco county, new construction plateaued in 2008\r\nbefore plummeting.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-07-05-sf-rents/sf-rents_files/figure-html5/fig2-1.png",
    "last_modified": "2022-07-06T00:12:27+01:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 1152
  },
  {
    "path": "posts/2022-01-26-chocolate-bar-ratings/",
    "title": "Text Mining Chocolate Bar Characteristics with {tidytext}",
    "description": "Graphs and analysis using the #TidyTuesday data set for week 3 of 2022\n  (18/1/2022): \"Chocolate Bar ratings\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2022-01-26",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSetup\r\nData wrangling\r\nPlotting the most used characteristics in the chocolate bar summaries\r\nPlotting the most important characteristics in the chocolate bar summaries\r\nReferences\r\n\r\nIntroduction\r\nIn this post, memorable characteristics of chocolate bars are plotted. These characteristics relate to anything about the bars, e.g. texture, flavour, overall opinion. This data set includes the country of cocoa bean origin for each chocolate bar, including “blend” for bars with multiple beans. To create these plots, the data set is filtered to select the six countries of origin with the most chocolate bar characteristics.\r\nThe first plot lists the top fifteen most used characteristics for bars from each country. The countries of origin cannot be distinguished based on this plot, most of the characteristics listed can be applied to all the chocolate bars in the data set. “Sweet” is listed in every plot, for example. To find characteristics that are unique or important to chocolate from each country, term frequency–inverse document frequency (tf-idf) can be used to select characteristics, instead of number of occurrences alone. In this case, the characteristics associated with each country are treated as separate documents. The second plot lists the top fifteen most characteristics for the same chocolate bars using tf-idf. From this plot, we can see characteristics that are often associated with one group of chocolate bars, but not the other groups. For example, chocolate bars made using a blend of beans are the most likely to list “poor after taste” as a characteristics, whereas “grape” is a characteristic most likely associated with Peruvian chocolate bars.\r\nSetup\r\nLoading the R libraries and data set.\r\n\r\n\r\n# Loading libraries\r\nlibrary(tidytuesdayR)\r\nlibrary(tidyverse)\r\nlibrary(ggthemes)\r\nlibrary(tidytext)\r\n\r\n# Loading data\r\ntt <- tt_load(\"2022-01-18\")\r\n\r\n\r\n\r\n    Downloading file 1 of 1: `chocolate.csv`\r\n\r\nData wrangling\r\n\r\n\r\n# Counting how many times each characteristic is used\r\ncountry_characteristics <- tt$chocolate %>%\r\n  unnest_tokens(memorable_characteristic,\r\n                most_memorable_characteristics, token = \"regex\",\r\n                pattern = \",\", to_lower = TRUE) %>%\r\n  mutate(memorable_characteristic = str_squish(memorable_characteristic)) %>%\r\n  count(country_of_bean_origin, memorable_characteristic, sort = TRUE)\r\n\r\n# Counting the total number of characteristics used for each country of origin\r\ntotal_country_characteristics <- country_characteristics %>%\r\n  group_by(country_of_bean_origin) %>%\r\n  summarise(total = sum(n))\r\n\r\n# Joining these data frames\r\ncountry_characteristics <- left_join(country_characteristics,\r\n                                     total_country_characteristics,\r\n                                     by = \"country_of_bean_origin\")\r\n\r\n# Finding the six countries of origin with the most characteristics\r\ntop_countries <- total_country_characteristics %>%\r\n  slice_max(n = 6, order_by = total) %>%\r\n  select(country_of_bean_origin)\r\n\r\n# Filtering the data\r\ncountry_characteristics <- country_characteristics %>%\r\n  filter(country_of_bean_origin %in% top_countries$country_of_bean_origin) %>%\r\n  select(country_of_bean_origin, memorable_characteristic, n, total)\r\n\r\n# Adding tf-idf\r\ncountry_characteristics <- country_characteristics %>%\r\n  bind_tf_idf(memorable_characteristic,\r\n              country_of_bean_origin, n)\r\n\r\n# Printing a summary of the data frame\r\ncountry_characteristics\r\n\r\n\r\n# A tibble: 1,233 x 7\r\n   country_of_bean_~ memorable_charac~     n total     tf   idf tf_idf\r\n   <chr>             <chr>             <int> <int>  <dbl> <dbl>  <dbl>\r\n 1 Venezuela         nutty                84   722 0.116      0      0\r\n 2 Ecuador           floral               72   600 0.12       0      0\r\n 3 Dominican Republ~ earthy               35   641 0.0546     0      0\r\n 4 Venezuela         roasty               34   722 0.0471     0      0\r\n 5 Venezuela         creamy               33   722 0.0457     0      0\r\n 6 Peru              cocoa                28   678 0.0413     0      0\r\n 7 Blend             sweet                27   443 0.0609     0      0\r\n 8 Madagascar        sour                 27   485 0.0557     0      0\r\n 9 Blend             cocoa                26   443 0.0587     0      0\r\n10 Dominican Republ~ cocoa                26   641 0.0406     0      0\r\n# ... with 1,223 more rows\r\n\r\nPlotting the most used characteristics in the chocolate bar summaries\r\n\r\n\r\n# Plotting the most used characteristics in the chocolate bar summaries\r\ncountry_characteristics %>%\r\n  group_by(country_of_bean_origin) %>%\r\n  slice_max(n, n = 15, with_ties = FALSE) %>%\r\n  ungroup() %>%\r\n  mutate(country_of_bean_origin = as.factor(country_of_bean_origin),\r\n         memorable_characteristic = reorder_within(memorable_characteristic,\r\n                                                   n,\r\n                                                   country_of_bean_origin)) %>%\r\n  ggplot(aes(n, memorable_characteristic, fill = country_of_bean_origin)) +\r\n  geom_col(show.legend = FALSE) +\r\n  scale_y_reordered() +\r\n  theme_solarized_2() +\r\n  facet_wrap(~country_of_bean_origin, ncol = 2, scales = \"free\") +\r\n  labs(title = \"Most used Characteristics in Chocolate Bar Summaries\",\r\n       subtitle = \"Summaries grouped by cocoa country of origin\",\r\n       x = \"Characteristic count\", y = NULL)\r\n\r\n\r\n\r\n\r\nFigure 1: Characteristics that are most used to describe chocolate bars made using different cocoa beans are plotted.\r\n\r\n\r\n\r\nPlotting the most important characteristics in the chocolate bar summaries\r\n\r\n\r\n# Plotting the most important characteristics in the chocolate bar summaries\r\ncountry_characteristics %>%\r\n  group_by(country_of_bean_origin) %>%\r\n  slice_max(tf_idf, n = 15, with_ties = FALSE) %>%\r\n  ungroup() %>%\r\n  mutate(country_of_bean_origin = as.factor(country_of_bean_origin),\r\n         memorable_characteristic = reorder_within(memorable_characteristic,\r\n                                                   tf_idf,\r\n                                                   country_of_bean_origin)) %>%\r\n  ggplot(aes(tf_idf, memorable_characteristic, fill = country_of_bean_origin)) +\r\n  geom_col(show.legend = FALSE) +\r\n  scale_y_reordered() +\r\n  theme_solarized_2() +\r\n  facet_wrap(~country_of_bean_origin, ncol = 2, scales = \"free\") +\r\n  labs(title = \"Important Characteristics in Chocolate Bar Summaries\",\r\n       subtitle = \"Reviews grouped by cocoa country of origin\",\r\n       x = \"Term frequency–inverse document frequency (tf-idf)\", y = NULL)\r\n\r\n\r\n\r\n\r\nFigure 2: Characteristics that are often used to describe chocolate bars made using cocoa from a given country, but not for other chocolate bars, are plotted.\r\n\r\n\r\n\r\nReferences\r\nAnalyzing word and document frequency: tf-idf\r\nFaceting and Reordering with ggplot2\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-26-chocolate-bar-ratings/chocolate-bar-ratings_files/figure-html5/fig2-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 2304
  },
  {
    "path": "posts/2022-01-23-bee-colony-losses/",
    "title": "Plotting Bee Colony Observations and Distributions using {ggbeeswarm} and {geomtextpath}",
    "description": "Graphs and analysis using the #TidyTuesday data set for week 2 of 2022\n  (11/1/2022): \"Bee Colony losses\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2022-01-23",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSetup\r\nData wrangling\r\nPlotting Bee Colony observations using {ggbeeswarm}\r\nAnimating Bee Colony observations over time\r\nPlotting the distribution of different Bee Colony observation types\r\n\r\nSetup\r\nLoading the R libraries and data set.\r\n\r\n\r\n# Loading libraries\r\nlibrary(geomtextpath) # For adding text to ggplot2 curves\r\nlibrary(tidytuesdayR) # For loading data set\r\nlibrary(ggbeeswarm) # For creating a beeswarm plot\r\nlibrary(tidyverse) # For the ggplot2, dplyr libraries\r\nlibrary(gganimate) # For plot animation\r\nlibrary(ggthemes) # For more ggplot2 themes\r\nlibrary(viridis) # For plot themes\r\n\r\n# Loading data set\r\ntt <- tt_load(\"2022-01-11\")\r\n\r\n\r\n\r\n    Downloading file 1 of 2: `colony.csv`\r\n    Downloading file 2 of 2: `stressor.csv`\r\n\r\nData wrangling\r\nIn this section, the Bee Colony data is wrangled into two tidy sets:\r\ntidied_colony_counts_overall contains quarterly colony counts for the USA\r\ntidied_colony_counts_per_state contains quarterly colony counts for various states within the USA\r\nTo create these sets, the original data is filtered to select for the appropriate states, and the “tidy_colony_data()” function is applied. These sets are tidy as each column is a variable, each row is an observation, and every cell has a single value. The types of observations in these data sets are:\r\nTotal colonies: Bee colonies counted\r\nLost: Bee colonies lost\r\nAdded: Bee colonies added\r\nRenovated: Bee colonies renovated\r\n\r\n\r\n# Creating subsets of the original bee colony data\r\ncolony_counts_overall <- tt$colony %>%\r\n  filter(state == \"United States\")\r\n\r\ncolony_counts_per_state <- tt$colony %>%\r\n  filter(state != \"United States\" & state != \"Other states\")\r\n\r\n# Defining a function to tidy bee colony count data, which takes\r\n# \"messy_colony_data\" as an argument\r\ntidy_colony_data <- function(messy_colony_data){\r\n  # Writing the result of the following piped steps to \"tidied_colony_data\"\r\n  tidied_colony_data <- messy_colony_data %>%\r\n    # Selecting variables\r\n    select(year, colony_n, colony_lost, colony_added, colony_reno) %>%\r\n    # Dropping rows with missing values\r\n    drop_na() %>%\r\n    # Changing columns to rows\r\n    pivot_longer(!year, names_to = \"type\", values_to = \"count\") %>%\r\n    # Setting \"type\" as a factor variable\r\n    mutate(type = factor(type)) %>%\r\n    # Recoding the levels of the \"type\" factor\r\n    mutate(type = fct_recode(type,\r\n                             \"Total colonies\" = \"colony_n\",\r\n                             \"Lost\" = \"colony_lost\",\r\n                             \"Added\" = \"colony_added\",\r\n                             \"Renovated\" = \"colony_reno\")) %>%\r\n    # Reordering \"type\" factor levels\r\n    mutate(type = fct_relevel(type,\r\n                              \"Total colonies\", \"Lost\", \"Added\", \"Renovated\"))\r\n  # Returning \"tidied_colony_data\"\r\n  return(tidied_colony_data)\r\n}\r\n\r\n# Using this function to tidy the subsets\r\ntidied_colony_counts_overall <- tidy_colony_data(colony_counts_overall)\r\n\r\ntidied_colony_counts_per_state <- tidy_colony_data(colony_counts_per_state)\r\n\r\n# Printing a summary of the subsets before tidying...\r\ncolony_counts_overall\r\n\r\n\r\n# A tibble: 26 x 10\r\n    year months  state colony_n colony_max colony_lost colony_lost_pct\r\n   <dbl> <chr>   <chr>    <dbl>      <dbl>       <dbl>           <dbl>\r\n 1  2015 Januar~ Unit~  2824610         NA      500020              18\r\n 2  2015 April-~ Unit~  2849500         NA      352860              12\r\n 3  2015 July-S~ Unit~  3132880         NA      457100              15\r\n 4  2015 Octobe~ Unit~  2874760         NA      412380              14\r\n 5  2016 Januar~ Unit~  2594590         NA      428800              17\r\n 6  2016 April-~ Unit~  2801470         NA      329820              12\r\n 7  2016 July-S~ Unit~  3181180         NA      397290              12\r\n 8  2016 Octobe~ Unit~  3032060         NA      502350              17\r\n 9  2017 Januar~ Unit~  2615590         NA      361850              14\r\n10  2017 April-~ Unit~  2886030         NA      225680               8\r\n# ... with 16 more rows, and 3 more variables: colony_added <dbl>,\r\n#   colony_reno <dbl>, colony_reno_pct <dbl>\r\n\r\ncolony_counts_per_state\r\n\r\n\r\n# A tibble: 1,196 x 10\r\n    year months  state colony_n colony_max colony_lost colony_lost_pct\r\n   <dbl> <chr>   <chr>    <dbl>      <dbl>       <dbl>           <dbl>\r\n 1  2015 Januar~ Alab~     7000       7000        1800              26\r\n 2  2015 Januar~ Ariz~    35000      35000        4600              13\r\n 3  2015 Januar~ Arka~    13000      14000        1500              11\r\n 4  2015 Januar~ Cali~  1440000    1690000      255000              15\r\n 5  2015 Januar~ Colo~     3500      12500        1500              12\r\n 6  2015 Januar~ Conn~     3900       3900         870              22\r\n 7  2015 Januar~ Flor~   305000     315000       42000              13\r\n 8  2015 Januar~ Geor~   104000     105000       14500              14\r\n 9  2015 Januar~ Hawa~    10500      10500         380               4\r\n10  2015 Januar~ Idaho    81000      88000        3700               4\r\n# ... with 1,186 more rows, and 3 more variables: colony_added <dbl>,\r\n#   colony_reno <dbl>, colony_reno_pct <dbl>\r\n\r\n# ...and after tidying\r\ntidied_colony_counts_overall\r\n\r\n\r\n# A tibble: 100 x 3\r\n    year type             count\r\n   <dbl> <fct>            <dbl>\r\n 1  2015 Total colonies 2824610\r\n 2  2015 Lost            500020\r\n 3  2015 Added           546980\r\n 4  2015 Renovated       270530\r\n 5  2015 Total colonies 2849500\r\n 6  2015 Lost            352860\r\n 7  2015 Added           661860\r\n 8  2015 Renovated       692850\r\n 9  2015 Total colonies 3132880\r\n10  2015 Lost            457100\r\n# ... with 90 more rows\r\n\r\ntidied_colony_counts_per_state\r\n\r\n\r\n# A tibble: 4,208 x 3\r\n    year type           count\r\n   <dbl> <fct>          <dbl>\r\n 1  2015 Total colonies  7000\r\n 2  2015 Lost            1800\r\n 3  2015 Added           2800\r\n 4  2015 Renovated        250\r\n 5  2015 Total colonies 35000\r\n 6  2015 Lost            4600\r\n 7  2015 Added           3400\r\n 8  2015 Renovated       2100\r\n 9  2015 Total colonies 13000\r\n10  2015 Lost            1500\r\n# ... with 4,198 more rows\r\n\r\nPlotting Bee Colony observations using {ggbeeswarm}\r\nThe first graph plots a point for each type of observation using geom_beeswarm().\r\n\r\n\r\n# Plotting Bee Colony observations using geom_beeswarm() from {ggbeeswarm}\r\ntidied_colony_counts_per_state %>%\r\n  ggplot(aes(x = type, y = count)) +\r\n  geom_beeswarm(cex = 4, colour = \"yellow\") +\r\n  scale_y_log10() +\r\n  theme_solarized_2(light = FALSE) +\r\n  facet_wrap(~type, scales = \"free\") +\r\n  theme(legend.position=\"none\", axis.text.x = element_blank()) +\r\n  labs(title = \"Bee Colonies Counted, Lost, Added, Renovated\",\r\n       subtitle = \"Created using {ggbeeswarm}\",\r\n       x = NULL, y = \"Number of bee colonies (log10)\",\r\n       fill = NULL)\r\n\r\n\r\n\r\n\r\nFigure 1: Scatter plots of bee colony observations. This plot has a point for each observation. Points are jittered to reduce overplotting.\r\n\r\n\r\n\r\nAnimating Bee Colony observations over time\r\nWhile the previous plot is thematically appropriate, it could be better. This graph plots the same points over time in an animation, with the year plotted given in the subtitle. This graph uses standard {ggplot2} jittered points, as well as a box plot to illustrate the distribution of the points. These box plots have notches, showing 95% confidence intervals for the median. Distributions with notches that do not overlap differ significantly.\r\n\r\n\r\n# Defining an animation showing bee colony counts over time\r\np <- tidied_colony_counts_per_state %>%\r\n  ggplot(aes(x = count, y = fct_reorder(type, count))) +\r\n  geom_jitter(color = \"yellow\", alpha = 0.8) +\r\n  geom_boxplot(width = 0.2, alpha = 0.8, notch = TRUE, colour = \"cyan\") +\r\n  scale_x_log10() +\r\n  theme_solarized_2(light = FALSE) +\r\n  theme(legend.position=\"none\", axis.ticks.y = element_blank(),\r\n        axis.line.y = element_blank()) +\r\n  transition_time(as.integer(year)) +\r\n  labs(title = \"Bee Colonies Counted, Lost, Added, Renovated, per year\",\r\n       subtitle = \"Year: {frame_time}\",\r\n       x = \"Number of bee colonies (log10)\", y = NULL)\r\n\r\n# Rendering the animation as a .gif\r\nanimate(p, nframes = 180, start_pause = 20,  end_pause = 20,\r\n        renderer = magick_renderer())\r\n\r\n\r\n\r\n\r\nFigure 2: Animation showing bee colony counts from 2015 to 2021.\r\n\r\n\r\n\r\nPlotting the distribution of different Bee Colony observation types\r\nFrom the previous plot, we can see that the Added and Renovated variables have similar distributions based on their box plots. Distributions can also be visualised using density plots. In this graph, the distribution of different types of observation in the data set are plotted.\r\n\r\n\r\n# Creating a density plot for different observation types\r\ntidied_colony_counts_overall %>%\r\n  filter(type != \"Total colonies\") %>%\r\n  ggplot(aes(x = count, colour = type, label = type)) +\r\n  geom_textdensity(size = 7, fontface = 2, hjust = 0.89, vjust = 0.3,\r\n                   linewidth = 1.2) +\r\n  theme_solarized_2(light = FALSE) +\r\n  theme(legend.position = \"none\") +\r\n  labs(title = \"Distribution of Bee Colony Counts\",\r\n       subtitle = \"Distributions of Bee Colonies Addded, Renovated, Lost\",\r\n       x = \"Number of bee colonies\")\r\n\r\n\r\n\r\n\r\nFigure 3: A density plot, giving the distribution of various observations. Of the three types of observation plotted, Added and Renovated are the most similar.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-23-bee-colony-losses/bee-colony-losses_files/figure-html5/fig3-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 1152
  },
  {
    "path": "posts/2021-08-18-star-trek-voice-commands/",
    "title": "Text mining Star Trek dialogue and classifying characters using machine learning",
    "description": "Graphs, text mining and analysis using the #TidyTuesday data set for week 34 of 2021\n  (17/8/2021): \"Star Trek voice commands\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2021-08-18",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSetup\r\nMost spoken words by prominent characters\r\nWords specific to prominent characters\r\nSpecifying a model to classify People and Computers in the data set\r\nFitting the model to resampled training data\r\nEvaluating the model using the test data\r\nImportant words in the final model\r\nReferences\r\n\r\nIntroduction\r\nIn this article, the Star Trek voice commands data set from the #TidyTuesday project is used to investigate character diction using text mining, and train a machine learning model to distinguish between people and computers in the data set. The techniques used in this article are taken from the following textbooks, both of which are freely available online:\r\nText Mining with R\r\nSupervised Machine Learning for Text Analysis in R\r\nSetup\r\nLoading the R libraries and data set.\r\n\r\n\r\n# Loading libraries\r\nlibrary(tidytuesdayR) # For loading data set\r\nlibrary(textrecipes) # For adding tf-idf to recipies (step_tfidf)\r\nlibrary(tidyverse) # For the ggplot2, dplyr, forcats libraries\r\nlibrary(tidytext) # For text mining\r\nlibrary(glmnet) # For lasso model\r\nlibrary(tidymodels) # For various modelling libraries\r\n\r\n# Loading data set\r\ntt <- tt_load(\"2021-08-17\")\r\n\r\n\r\n\r\n    Downloading file 1 of 1: `computer.csv`\r\n\r\nMost spoken words by prominent characters\r\nThe first graph plots the words most frequently used by prominent characters when talking to computers. They were chosen by selecting characters in the data set that appeared more than 150 times each.\r\n\r\n\r\n# Creating a vector of characters with most interactions with computers\r\nchars_with_most_interactions <- tt$computer %>%\r\n  filter(char_type != \"Computer\") %>% # Filtering out rows for computer lines\r\n  count(char, sort = TRUE) %>% # Counting the number of rows for each character\r\n  filter(n >= 150) %>% # Filtering out characters with under 150 lines\r\n  pull(char) # Pulling the names of the characters as a vector\r\n\r\n# Counting words spoken per character\r\ncharacter_words <- tt$computer %>%\r\n  filter(char %in% chars_with_most_interactions) %>% # Selecting rows\r\n  unnest_tokens(word, interaction) %>% # Creating a row per word in interaction\r\n  anti_join(get_stopwords(), by = \"word\") %>% # Removing stop words\r\n  count(char, word, sort = TRUE) # Counting words per character\r\n\r\n# Plotting most spoken words per character\r\ncharacter_words %>%\r\n  group_by(char) %>%\r\n  slice_max(n, n = 15, with_ties = FALSE) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(n, fct_reorder(word, n), fill = char)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~char, ncol = 2, scales = \"free\") +\r\n  scale_fill_viridis_d() +\r\n  theme_classic() +\r\n  labs(title = \"Words most frequently spoken by prominent characters\",\r\n       x = \"Utterances\", y = \"Words\")\r\n\r\n\r\n\r\n\r\nFigure 1: The most frequently spoken word is computer.\r\n\r\n\r\n\r\nWords specific to prominent characters\r\nFrom Figure 1, we can see that the most frequently spoken word across all four characters is “computer”. This is not surprising, as this data set records interactions between characters and computers, with characters addressing the computers as “computer”. An expected result, but it does not tell us anything about the characters. What if we wanted to see which words were specific to each character?\r\nTo see which words are specific to these characters, we can use term frequency-inverse document frequency (tf-idf) to measure word importance. For a given word, tf-idf increases the more often it is used, and decreases relative to the number of documents it appears in. In this case, each “document” refers to a character. So even though “computer” is the most frequent term used by the characters, it will have a low tf-idf value as it is used by every character (therefore it appears in every document).\r\n\r\n\r\n# Counting total number of words spoken per character\r\ntotal_words <- character_words %>% \r\n  group_by(char) %>%  # Grouping words by character\r\n  summarize(total = sum(n)) # Counting words spoken per character (\"total\")\r\n\r\n# Joining tbl_dfs: adding \"total\" variable to character_words\r\ncharacter_words <- left_join(character_words, total_words)\r\n\r\n# Adding tf_idf to character_words\r\ncharacter_words <- character_words %>%\r\n  bind_tf_idf(term = word, document = char, n = n) %>%\r\n  arrange(desc(tf_idf)) # Arranging rows by descending tf_idf values\r\n\r\n# Printing a summary of the character_words object\r\nglimpse(character_words)\r\n\r\n\r\nRows: 1,492\r\nColumns: 7\r\n$ char   <chr> \"Riker\", \"Riker\", \"Riker\", \"Riker\", \"Riker\", \"Riker\",…\r\n$ word   <chr> \"little\", \"omega\", \"indeed\", \"jazz\", \"good\", \"audienc…\r\n$ n      <int> 18, 15, 14, 14, 24, 12, 12, 12, 12, 12, 12, 12, 12, 1…\r\n$ total  <int> 1144, 1144, 1144, 1144, 1144, 1144, 1144, 1144, 1144,…\r\n$ tf     <dbl> 0.015734266, 0.013111888, 0.012237762, 0.012237762, 0…\r\n$ idf    <dbl> 1.3862944, 1.3862944, 1.3862944, 1.3862944, 0.6931472…\r\n$ tf_idf <dbl> 0.02181232, 0.01817694, 0.01696514, 0.01696514, 0.014…\r\n\r\n# Plotting words with the top tf-idf values per character\r\ncharacter_words %>%\r\n  group_by(char) %>%\r\n  slice_max(tf_idf, n = 15, with_ties = FALSE) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = char)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~char, ncol = 2, scales = \"free\") +\r\n  scale_fill_viridis_d() +\r\n  theme_classic() +\r\n  labs(title = \"Spoken words specific to each character\",\r\n       subtitle = \"Words listed are used often by each character relative to other characters\",\r\n       y = \"Words\",\r\n       x = \"term frequency-inverse document frequency (tf-idf)\")\r\n\r\n\r\n\r\n\r\nFigure 2: Data is the most likely character to say please when addressing a computer. Riker likes jazz. Picard likes to say his own name\r\n\r\n\r\n\r\nSpecifying a model to classify People and Computers in the data set\r\nIn this section, a machine learning model for classifying interactions into categories is set up. The variable “char_type” divides the data set into two categories: the lines spoken by people to computers (char_type = “Person”) and responses from the computers (char_type = “Computer”). The goal of this model is to look at the dialogue in the “interaction” variable, and determine whether that dialogue was spoken by a “Person” or “Computer” character based on the words used. To do this:\r\nThe data set is split into a subset for training the machine learning model and a subset for testing the model\r\nThe training data is split into ten folds, each with its own training and testing sections (10-fold cross-validation)\r\nA number of preprocessing steps for the data set are defined\r\nThe type of machine learning model is defined (a lasso regularised model)\r\nThe preprocessing steps and model type are combined into a workflow\r\nThe preprocessing for this data set includes a downsampling step. There is a class imbalance in this dataset, as there are over twice the number of “Person” interactions as “Computer” ones. Including the downsampling step addresses this class imbalance by removing observations of the majority class (“Person”) during training.\r\n\r\n\r\n# Summarising the values in the char_type variable\r\ntt$computer %>% select(char_type) %>% table()\r\n\r\n\r\n.\r\nComputer   Person \r\n     708     1506 \r\n\r\n# Setting seed for reproducibility\r\nset.seed(20210818)\r\n# Splitting data into training and test subsets\r\nchar_type_split <- tt$computer %>%\r\n  initial_split(strata = char_type) # Ensures equal proportion of char_type\r\n\r\n# Creating training and test sets using departures_split\r\nchar_type_train <- training(char_type_split)\r\nchar_type_test <- testing(char_type_split)\r\n\r\n# Setting seed for reproducibility\r\nset.seed(20210818)\r\n# Setting up 10-fold cross-validation (CV) using training data\r\nchar_type_folds <- vfold_cv(char_type_train,\r\n                            strata = char_type) # Ensures equal proportion of char_type\r\n\r\n# Creating preprocessing recipe for predicting if lines are spoken by a computer\r\nchar_type_rec <-  recipe(char_type ~ interaction, data = char_type_train) %>%\r\n  step_tokenize(interaction) %>% # Splitting interaction variable into words\r\n  step_tokenfilter(interaction, max_tokens = 1500) %>% # Limiting tokens used\r\n  step_tfidf(interaction) %>% # Weighing tokens by tf-idf\r\n  themis::step_downsample(char_type) # Downsampling to address class imbalance\r\n\r\n# Specifying a lasso regularised model\r\nlasso_spec <- logistic_reg(penalty = 0.01, mixture = 1) %>%\r\n  set_mode(\"classification\") %>%\r\n  set_engine(\"glmnet\")\r\n\r\n# Creating a supervised machine learning workflow\r\nchar_type_wf <- workflow() %>%\r\n  add_recipe(char_type_rec) %>%\r\n  add_model(lasso_spec)\r\n\r\n# Printing workflow\r\nchar_type_wf\r\n\r\n\r\n══ Workflow ══════════════════════════════════════════════════════════\r\nPreprocessor: Recipe\r\nModel: logistic_reg()\r\n\r\n── Preprocessor ──────────────────────────────────────────────────────\r\n4 Recipe Steps\r\n\r\n• step_tokenize()\r\n• step_tokenfilter()\r\n• step_tfidf()\r\n• step_downsample()\r\n\r\n── Model ─────────────────────────────────────────────────────────────\r\nLogistic Regression Model Specification (classification)\r\n\r\nMain Arguments:\r\n  penalty = 0.01\r\n  mixture = 1\r\n\r\nComputational engine: glmnet \r\n\r\nFitting the model to resampled training data\r\nNow that the machine learning workflow is in place, the model can be evaluated using the training data. It will be fit to the training model ten times, as specified in the 10-fold cross-validation above. As each of these folds tests the model using a different set of observations for training and testing, it gives a more accurate estimate of the performance of the model than just training and testing once.\r\nTwo metrics are used to estimate model performance:\r\naccuracy is the proportion of the data that is predicted correctly. The closer to 1 (100%), the better\r\nROC AUC (Receiver Operating Characteristic Area Under Curve) measures how well a classifier performs at different thresholds. The closer to 1, the better. A ROC AUC closer to 0.5 indicates that the model is no better than random guessing\r\nThe ROC curve is plotted for each of the training data resamples. The ROC curve is a plot of true positive rate (observations given the correct class) versus false positive rate. Essentially, the closer the ROC curve is to the top-right of the plot, the higher its true positive rate, the lower its false positive rate, the larger the area under the curve. If the machine learning model was just guessing, it would have a 50/50 chance of classifying a given observation correctly: this would result in a ROC curve that is a straight line through the origin, with an AUC of 0.5 (50%).\r\n\r\n\r\n# Fitting model to resampled folds to estimate performance\r\nchar_type_rs <- fit_resamples(\r\n  object = char_type_wf, # Machine learning workflow\r\n  resamples = char_type_folds, # 10-fold cross-validation\r\n  control = control_resamples(save_pred = TRUE)\r\n)\r\n\r\n\r\n\r\n\r\n\r\n# Collecting predictions from resampled folds\r\nchar_type_rs_predictions <- collect_predictions(char_type_rs)\r\n\r\n# Collecting model performance metrics metrics\r\nchar_type_rs_metrics <- collect_metrics(char_type_rs)\r\n# Printing performance metrics\r\nchar_type_rs_metrics\r\n\r\n\r\n# A tibble: 2 x 6\r\n  .metric  .estimator  mean     n std_err .config             \r\n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 accuracy binary     0.880    10 0.00650 Preprocessor1_Model1\r\n2 roc_auc  binary     0.952    10 0.00420 Preprocessor1_Model1\r\n\r\n# Plotting ROC curves for resampled folds\r\nchar_type_rs_predictions %>%\r\n  group_by(id) %>%\r\n  roc_curve(truth = char_type, .pred_Computer) %>%\r\n  autoplot() + \r\n  scale_color_viridis_d() +\r\n  labs(color = \"Resample (fold)\",\r\n       title = \"ROC curve for character type based on interaction\",\r\n       subtitle = \"True Positive Rate (TPR) v.s. False Positive Rate (FPR)\",\r\n       x = \"FPR (1 - Specificity)\", y = \"TPR (Sensitivity)\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nFigure 3: The machine learning model is far better at classifying characer types than random guessing.\r\n\r\n\r\n\r\nEvaluating the model using the test data\r\nFrom the 10-fold cross-validation training, we can see that this model is effective at classifying character type based on words used in interactions. To put this model to the test, we can use it to classify unseen data. Again, accuracy and ROC AUC will be used to evaluate model performance on the test data. A confusion matrix will also be plotted to illustrate performance: this matrix summarises model predictions versus the true values for observations in the test data.\r\n\r\n\r\n# Fitting model to training data, evaluating performance on test data\r\nchar_type_last_fit <- last_fit(char_type_wf, char_type_split)\r\n# Printing metrics for final fit\r\ncollect_metrics(char_type_last_fit)\r\n\r\n\r\n# A tibble: 2 x 4\r\n  .metric  .estimator .estimate .config             \r\n  <chr>    <chr>          <dbl> <chr>               \r\n1 accuracy binary         0.895 Preprocessor1_Model1\r\n2 roc_auc  binary         0.952 Preprocessor1_Model1\r\n\r\n\r\n\r\n# Plotting confusion matrix of testing data\r\ncollect_predictions(char_type_last_fit) %>%\r\n  conf_mat(truth = char_type, estimate = .pred_class) %>%\r\n  autoplot(type = \"heatmap\") +\r\n  scale_fill_continuous() +\r\n  labs(title = \"Confusion matrix of lasso classification model performance on test subset\",\r\n       subtitle = \"Sum of True Positives and False Positives for both classes: Computer and Person\")\r\n\r\n\r\n\r\n\r\nFigure 4: The model correctly classifies most observations in the test data.\r\n\r\n\r\n\r\nImportant words in the final model\r\nThe model is effective at classifying observations in the test data, with an accuracy of 89% and a ROC AUC of 95%. At this point, we may want to know more about how the model works. For example, which words had the largest impact on the model’s decision to classify a character type as “Computer” or “Person”? To see which words were important in the final model, we can extract the coefficient assigned to each word in the final fit, and plot the words with the highest values.\r\n\r\n\r\n# Extracting model fit\r\nchar_type_workflow_fit <- pull_workflow_fit(char_type_last_fit$.workflow[[1]])\r\n\r\n# Visualising the most important words for predicting whether a line was spoken\r\n# by a Computer or Person character\r\ntidy(char_type_workflow_fit) %>%\r\n  filter(term != \"Bias\", term != \"(Intercept)\") %>%\r\n  group_by(sign = estimate > 0) %>%\r\n  slice_max(abs(estimate), n = 15) %>% \r\n  ungroup() %>%\r\n  mutate(term = str_remove(term, \"tfidf_interaction_\"), # Tidying terms\r\n         sign = ifelse(sign, \"More likely from a Person\",\r\n                       \"More likely a Computer\")) %>%\r\n  ggplot(aes(abs(estimate), fct_reorder(term, abs(estimate)), fill = sign)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~sign, scales = \"free\") +\r\n  theme_classic() +\r\n  scale_fill_viridis_d() +\r\n  labs(titles = \"Which words are more likely to be used by a Computer vs. Person character?\",\r\n       subtitle = \"Importance assigned by lasso classification model, based lines spoken\",\r\n       x = \"Coefficient from lasso regularised model\", y = \"Words\")\r\n\r\n\r\n\r\n\r\nFigure 5: The model’s most important word for classifying Person characters was computer.\r\n\r\n\r\n\r\nReferences\r\nText Mining with R, Chapter 3 Analyzing word and document frequency: tf-idf\r\nSupervised Machine Learning for Text Analysis in R, Chapter 7 Classification\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-18-star-trek-voice-commands/star-trek-voice-commands_files/figure-html5/fig4-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 960
  },
  {
    "path": "posts/2021-08-15-bea-infrastructure-investment/",
    "title": "Adjusting variable distribution and exploring data using mass linear regression",
    "description": "Graphs and analysis using the #TidyTuesday data set for week 33 of 2021\n  (10/8/2021): \"BEA Infrastructure Investment\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2021-08-15",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSetup\r\nPlotting distribution of inflation-adjusted infrastructure investments\r\nExploring a data set using mass linear regression\r\nReferences\r\n\r\nIntroduction\r\nIn this post, the BEA Infrastructure Investment data set from the #TidyTuesday project is used to illustrate variable transformation and the exploreR::masslm() function. The variable for gross infrastructure investment adjusted for inflation is transformed to make it less skewed. Using these transformed investment values, multiple linear models are then created to quickly see which variables in the data set have the largest impact on infrastructure investment.\r\nSetup\r\nLoading the R libraries and data set.\r\n\r\nShow code\r\n# Loading libraries\r\nlibrary(tidyverse)\r\nlibrary(tidytuesdayR)\r\nlibrary(exploreR)\r\n\r\n# Loading data\r\ntt <- tt_load(\"2021-08-10\")\r\n\r\n\r\n\r\n    Downloading file 1 of 3: `ipd.csv`\r\n    Downloading file 2 of 3: `chain_investment.csv`\r\n    Downloading file 3 of 3: `investment.csv`\r\n\r\nPlotting distribution of inflation-adjusted infrastructure investments\r\nIn this section, the gross infrastructure investment (chained 2021 dollars) in millions of USD are plotted with and without a \\(\\log{10}\\) transformation. From the histograms below, we can see that applying a \\(\\log{10}\\) transformation gives the variable a less skewed distribution. This transformation should be considered for statistical testing of inflation-adjusted infrastructure investments.\r\n\r\nShow code\r\n# Creating tbl_df with gross_inv_chain values\r\nuntransformed_tbl_df <- tibble(\r\n  gross_inv_chain = tt$chain_investment$gross_inv_chain,\r\n  transformation = \"Untransformed\"\r\n  )\r\n\r\n# Creating tbl_df with log10(gross_inv_chain) values\r\nlog10_tbl_df <- tibble(\r\n  gross_inv_chain = log10(tt$chain_investment$gross_inv_chain),\r\n  transformation = \"Log10\"\r\n)\r\n\r\n# Combining the above tibbles into one tbl_df\r\ngross_inv_chain_tbl_df <- rbind(untransformed_tbl_df, log10_tbl_df)\r\n\r\n# Plotting distribution of inflation-adjusted infrastructure investments\r\ngross_inv_chain_tbl_df %>%\r\n  ggplot(aes(x = gross_inv_chain, fill = transformation)) +\r\n  geom_histogram(show.legend = FALSE, position = \"identity\",\r\n                 bins = 12, colour = \"black\") +\r\n  facet_wrap(~transformation, scales = \"free\") +\r\n  labs(fill.position = \"none\", y = NULL,\r\n       x = \"Gross infrastructure investments adjusted for inflation (millions USD)\",\r\n       title = \"Distributions of untransformed and log transformed infrastructure investments\",\r\n       subtitle = \"Log transformed investments are more normally distributed\") +\r\n  scale_fill_brewer(palette = \"Set1\") +\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\n(#fig:figure_1)The transformed variable is more appropriate for parametric statistical tests.\r\n\r\n\r\n\r\nExploring a data set using mass linear regression\r\nIn this section, exploreR::masslm() is applied to a copy of the data set with \\(\\log{10}\\) transformed investment values. The masslm() function from the exploreR package quickly produces a linear model of the dependent variable and every other variable in the data set. It then returns a data frame containing the features of each linear model that are useful when selecting predictor variables:\r\nR squared The proportion of variation in the dependent (response) variable that is explained by the independent (predictor) variable.\r\np-value The statistical significance of the model. A p-value \\(\\lt 5\\%\\) is typically considered significant.\r\nThis function is useful for quickly determining which variables should be included in predictive models. Note that the data set used should satisfy the assumptions of linear models, including a normally distributed response variable. In this case, the \\(\\log{10}\\) transformed investment variable is close to normal.\r\nFrom this mass linear regression model, we can see that investment category is the single variable that explains the largest proportion of variation in \\(\\log{10}\\) investment; and the linear model with group number is the most significant, followed by year.\r\n\r\nShow code\r\n# Creating a copy of the chain_investment data set with log10 transformed\r\n# gross investment values\r\nchain_investment_df <- tt$chain_investment %>%\r\n  # Creating a log10 transformed copy of gross_inv_chain\r\n  mutate(gross_inv_transformed = log10(gross_inv_chain)) %>%\r\n  # Removing -Inf values\r\n  filter(gross_inv_transformed != -Inf) %>%\r\n  # Selecting variables to include in the data frame\r\n  select(category, meta_cat, group_num, year, gross_inv_transformed)\r\n\r\n# Applying mass linear regression\r\ntransformed_investment_masslm <- masslm(chain_investment_df,\r\n                                        dv.var = \"gross_inv_transformed\")\r\n\r\n# Printing the masslm results in order of R squared values (decreasing)\r\ntransformed_investment_masslm %>%\r\n  arrange(-R.squared)\r\n\r\n\r\n         IV Coefficient    P.value  R.squared\r\n1  category   -0.579900  8.471e-10 0.63754622\r\n2  meta_cat    0.349300  7.848e-10 0.37782201\r\n3 group_num   -0.058750 3.625e-204 0.14695670\r\n4      year    0.009507  7.007e-59 0.04377399\r\nShow code\r\n# Printing the masslm results in order of p-values\r\ntransformed_investment_masslm %>%\r\n  arrange(P.value)\r\n\r\n\r\n         IV Coefficient    P.value  R.squared\r\n1 group_num   -0.058750 3.625e-204 0.14695670\r\n2      year    0.009507  7.007e-59 0.04377399\r\n3  meta_cat    0.349300  7.848e-10 0.37782201\r\n4  category   -0.579900  8.471e-10 0.63754622\r\n\r\nReferences\r\nexploreR vignette: The How and Why of Simple Tools\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-15-bea-infrastructure-investment/bea-infrastructure-investment_files/figure-html5/figure_1-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 960
  },
  {
    "path": "posts/2021-04-27-ceo-departures/",
    "title": "Predicting voluntary CEO departures using machine learning",
    "description": "Graphs and analysis using the #TidyTuesday data set for week 18 of 2021\n  (27/4/2021): \"CEO Departures\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2021-04-27",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSummary\r\nSetup\r\nData wrangling\r\n\r\nSetting parameters for machine learning model\r\nCreating a model using the training subset\r\nCreating a model using the full data set\r\nConfusion matrices for training and full models\r\nSignificant words according to the final model\r\nReferences\r\n\r\nSummary\r\nIn this post, a machine learning model is created using this week’s #TidyTuesday data set: CEO Departures. This data set contains descriptions of thousands of CEO departures and reasons for these departures. After filtering missing values from the data set and simplifying the departure reasons to “Voluntary” and “Involuntary”, a machine learning model is trained to distinguish between these departure categories. The model used is a linear support-vector machine (SVM) instructed to categorise departures based on individual words used in the departure descriptions. The final model is limited by class imbalance in the data set, as most of the CEO departures were voluntary.\r\nSetup\r\n\r\nShow code\r\n# Loading libraries\r\nlibrary(tidyverse)\r\nlibrary(tidytuesdayR)\r\nlibrary(tidymodels)\r\nlibrary(textrecipes)\r\nlibrary(themis)\r\nlibrary(parsnip)\r\nlibrary(doParallel)\r\n\r\n# Installing the GitHub version of {parsnip} to use svm_linear()\r\n#devtools::install_github(\"tidymodels/parsnip\")\r\n\r\n# Loading data set\r\ntt <- tt_load(\"2021-04-27\")\r\n\r\n\r\n\r\n    Downloading file 1 of 1: `departures.csv`\r\n\r\nData wrangling\r\nWrangling data for machine learning and visualisation.\r\n\r\nShow code\r\n# Creating a new variable \"depart_vol\" to record CEO departures as\r\n## Voluntary or Involuntary\r\ndepartures <- tt$departures %>% mutate(depart_vol = factor(departure_code))\r\n# Setting descriptive factor levels for \"depart_vol\"\r\nlevels(departures$depart_vol) <- list(\r\n  \"Involuntary\" = levels(departures$depart_vol)[1:4],\r\n  \"Voluntary\" = levels(departures$depart_vol)[5:6],\r\n  \"Other\" = levels(departures$depart_vol)[7:9]\r\n  )\r\n\r\n# Filtering \"depart_vol\" and \"notes\" to a new object: \"depart_notes\"\r\ndepart_notes <- departures %>%\r\n  filter(!is.na(notes) & !is.na(depart_vol)) %>%\r\n  filter(depart_vol != \"Other\") %>%\r\n  select(depart_vol, notes)\r\n\r\n# Dropping empty factor levels. In this case, the level \"Other\" is empty\r\ndepart_notes$depart_vol <- droplevels(depart_notes$depart_vol)\r\n# Printing the number of Involuntary and Voluntary CEO departures with descriptions\r\ntable(depart_notes$depart_vol)\r\n\r\n\r\n\r\nInvoluntary   Voluntary \r\n       1694        3771 \r\n\r\nSetting parameters for machine learning model\r\nIn this section…\r\nThe filtered data set is split into training and test data\r\n10-fold cross-validation (CV) is set for the training subset to get a more accurate estimate of training performance\r\nFeature engineering parameters are set\r\nThe linear SVM model parameters are set\r\nFeature engineering and model parameters are combined into a workflow\r\nPerformance metrics are selected\r\n\r\nShow code\r\n# Setting seed for reproducibility\r\nset.seed(100)\r\n# Splitting depart_notes into training and test data\r\ndepartures_split <- depart_notes %>%\r\n  initial_split(strata = depart_vol)\r\n\r\n# Creating training and test sets using departures_split\r\ndepartures_train <- training(departures_split)\r\ndepartures_test <- testing(departures_split)\r\n\r\n# Setting seed for reproducibility\r\nset.seed(100)\r\n# Setting 10-fold cross-validation (CV) using training data\r\ndepartures_folds <- vfold_cv(departures_train, strata = depart_vol)\r\n# Printing 10-fold CV splits\r\ndepartures_folds\r\n\r\n\r\n#  10-fold cross-validation using stratification \r\n# A tibble: 10 x 2\r\n   splits             id    \r\n   <list>             <chr> \r\n 1 <split [3688/410]> Fold01\r\n 2 <split [3688/410]> Fold02\r\n 3 <split [3688/410]> Fold03\r\n 4 <split [3688/410]> Fold04\r\n 5 <split [3688/410]> Fold05\r\n 6 <split [3688/410]> Fold06\r\n 7 <split [3688/410]> Fold07\r\n 8 <split [3688/410]> Fold08\r\n 9 <split [3689/409]> Fold09\r\n10 <split [3689/409]> Fold10\r\nShow code\r\n# Feature engineering\r\n## Creating preprocessing recipe for predicting voluntary/involuntary departures\r\ndepartures_rec <-  recipe(depart_vol ~ notes, data = departures_train) %>%\r\n  # Splitting the \"notes\" variable into individual tokens (words)\r\n  step_tokenize(notes) %>%\r\n  # Limiting number of tokens used in model\r\n  step_tokenfilter(notes, max_tokens = 1000) %>%\r\n  # Weighing tokens by tf-idf\r\n  step_tfidf(notes) %>%\r\n  # Normalising numeric predictors\r\n  step_normalize(all_numeric_predictors()) %>%\r\n  # Upsampling to address class imbalance\r\n  step_smote(depart_vol)\r\n\r\n# Specifying the machine learning model\r\n## This model will use a linear Support Vector Machine (SVM)\r\nsvm_spec <- svm_linear() %>%\r\n  set_mode(\"classification\") %>%\r\n  set_engine(\"LiblineaR\")\r\n\r\n# Combining the feature engineering recipe and model spec to create a workflow\r\ndepartures_wf <- workflow() %>%\r\n  add_recipe(departures_rec) %>%\r\n  add_model(svm_spec)\r\n# Printing the workflow\r\ndepartures_wf\r\n\r\n\r\n══ Workflow ══════════════════════════════════════════════════════════\r\nPreprocessor: Recipe\r\nModel: svm_linear()\r\n\r\n── Preprocessor ──────────────────────────────────────────────────────\r\n5 Recipe Steps\r\n\r\n• step_tokenize()\r\n• step_tokenfilter()\r\n• step_tfidf()\r\n• step_normalize()\r\n• step_smote()\r\n\r\n── Model ─────────────────────────────────────────────────────────────\r\nLinear Support Vector Machine Specification (classification)\r\n\r\nComputational engine: LiblineaR \r\nShow code\r\n# Selecting the metrics used for modelling\r\ndepartures_metrics <- metric_set(accuracy, recall, precision)\r\n\r\n\r\n\r\nCreating a model using the training subset\r\nIn this section, a linear SVM model is created using the training subset with 10-fold CV.\r\n\r\nShow code\r\n# Setting up parallel processing\r\ncl <- makePSOCKcluster(detectCores() - 1)\r\nregisterDoParallel(cl)\r\n\r\n# Setting seed for reproducibility\r\nset.seed(100)\r\n# Fitting models using resampling\r\nsvm_results <- fit_resamples(\r\n  departures_wf, # Specifying workflow\r\n  departures_folds, # Specifying cross validation folds\r\n  metrics = departures_metrics, # Metrics used for modelling\r\n  control = control_resamples(save_pred = TRUE) # Saving predictions for confusion matrix\r\n)\r\n\r\n# Printing performance metrics of SVM model\r\ncollect_metrics(svm_results)\r\n\r\n\r\n# A tibble: 3 x 6\r\n  .metric   .estimator  mean     n std_err .config             \r\n  <chr>     <chr>      <dbl> <int>   <dbl> <chr>               \r\n1 accuracy  binary     0.804    10 0.00592 Preprocessor1_Model1\r\n2 precision binary     0.677    10 0.0102  Preprocessor1_Model1\r\n3 recall    binary     0.706    10 0.00925 Preprocessor1_Model1\r\n\r\nCreating a model using the full data set\r\nIn this section, the final linear SVM model is created using the full data set.\r\n\r\nShow code\r\n# Setting seed for reproducibility\r\nset.seed(100)\r\n\r\n# Fitting the final model\r\nfinal_fitted <- last_fit(departures_wf, departures_split,\r\n                      metrics = departures_metrics)\r\n\r\n# Printing performance metrics of final fitted model\r\ncollect_metrics(final_fitted)\r\n\r\n\r\n# A tibble: 3 x 4\r\n  .metric   .estimator .estimate .config             \r\n  <chr>     <chr>          <dbl> <chr>               \r\n1 accuracy  binary         0.813 Preprocessor1_Model1\r\n2 recall    binary         0.708 Preprocessor1_Model1\r\n3 precision binary         0.694 Preprocessor1_Model1\r\n\r\nConfusion matrices for training and full models\r\nIn this section, confusion matrices are printed for the linear SVM models created using the training subset and full data set. The confusion matrix for the final model is plotted to illustrate the differences between this model’s predictions and the true values in the data set. From this plot, we can see that there is a class imbalance in the data set: most of the CEO departures with descriptions were Voluntary. As a result, Voluntary CEO departures are more accurately predicted based on event descriptions than Involuntary departures. Class imbalances can be addressed by upsampling (creating new examples of the minority class), which is used in the feature engineering of this model. Ideally, this data set would have more descriptions of Involuntary CEO departures.\r\n\r\nShow code\r\n# Printing the confusion matrix of the model trained using the 10-fold cv\r\n## training data\r\nsvm_results %>%\r\n  conf_mat_resampled(tidy = FALSE)\r\n\r\n\r\n            Involuntary Voluntary\r\nInvoluntary        89.7      42.9\r\nVoluntary          37.3     239.9\r\nShow code\r\n# Printing the confusion matrix of the final fitted model\r\ncollect_predictions(final_fitted) %>%\r\n  conf_mat(depart_vol, .pred_class)\r\n\r\n\r\n             Truth\r\nPrediction    Involuntary Voluntary\r\n  Involuntary         300       132\r\n  Voluntary           124       811\r\nShow code\r\n# Visualising confusion matrix of the final fitted model\r\ncollect_predictions(final_fitted) %>%\r\n  conf_mat(depart_vol, .pred_class) %>%\r\n  autoplot() +\r\n  labs(title = \"Prediction vs Truth for final linear SVM model\",\r\n       subtitle = \"Class imbalance is an issue; most of the observations were of Voluntary CEO departures\")\r\n\r\n\r\n\r\n\r\nFigure 1: Confusion matrix for the final linear SVM model.\r\n\r\n\r\n\r\nSignificant words according to the final model\r\nIn this section, the most important words for predicting whether a CEO departure was Involuntary or Voluntary are visualised. This is based on the coefficients from the final linear SVM model: words with higher predictive value are given coefficients with larger absolute values.\r\nFrom this plot, we can see that descriptions of Involuntary CEO departures are more likely to include the words “health”, “died”, “charge” and “dropped”. Descriptions of Voluntary CEO departures are more likely to include the words “retired”, “voluntarily”, “join” and “consulting”. Involuntary CEO departures described in the data set are likely due to poor health, death and litigation; Voluntary CEO departures are likely due to retirement, former executives joining other companies, or moving into consulting.\r\n\r\nShow code\r\n# Extracting model fit\r\ndepartures_fit <- pull_workflow_fit(final_fitted$.workflow[[1]])\r\n\r\n# Visualising the most important words for predicting whether a CEO departure\r\n## was voluntary\r\ntidy(departures_fit) %>%\r\n  filter(term != \"Bias\") %>%\r\n  group_by(sign = estimate > 0) %>%\r\n  slice_max(abs(estimate), n = 15) %>% \r\n  ungroup() %>%\r\n  mutate(term = str_remove(term, \"tfidf_notes_\"), # Tidying terms\r\n         sign = ifelse(sign, \"More likely from Voluntary CEO departures\",\r\n                       \"More likely from Involuntary CEO departures\")) %>%\r\n  ggplot(aes(abs(estimate), fct_reorder(term, abs(estimate)), fill = sign)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~sign, scales = \"free\") +\r\n  labs(x = \"Coefficient from linear SVM\", y = NULL) +\r\n  scale_fill_brewer(palette = \"Set1\") +\r\n  theme_classic() +\r\n  labs(titles = \"Which words are more likely to be used when a CEO leaves Involuntarily vs. Voluntarily?\",\r\n       subtitle = \"Importance assigned by linear SVM model, based on descriptions of CEO departures\")\r\n\r\n\r\n\r\n\r\nFigure 2: Words most likely to appear in CEO departure descriptions in each category, according to the linear SVM model.\r\n\r\n\r\n\r\nReferences\r\n“Which #TidyTuesday Netflix titles are movies and which are TV shows?” by Julia Silge\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-27-ceo-departures/ceo-departures_files/figure-html5/fig2-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 960
  },
  {
    "path": "posts/2021-04-21-netflix-titles/",
    "title": "Films with MPA ratings on Netflix",
    "description": "Graphs and analysis using the #TidyTuesday data set for week 17 of 2021\n  (20/4/2021): \"Netflix Titles\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2021-04-21",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSetup\r\nFilms for younger audiences tend to be shorter\r\nNetflix is adding fewer films for kids\r\nSignificant words in category descriptions\r\n\r\nSetup\r\nLoading the R libraries and data set.\r\n\r\nShow code\r\n# Loading libraries\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(tidytuesdayR)\r\nlibrary(forcats)\r\n\r\n# Loading data set\r\ntt <- tt_load(\"2021-04-20\")\r\n\r\n\r\n\r\n    Downloading file 1 of 1: `netflix_titles.csv`\r\n\r\nWrangling data for visualisation.\r\n\r\nShow code\r\n# Selecting the films in the data set, creating a numeric variable for film\r\n## duration called \"runtime\", creating a numeric variable for year added\r\nnetflix_movies <- tt$netflix_titles %>%\r\n  filter(type == \"Movie\") %>%\r\n  mutate(runtime = as.numeric(str_sub(duration, end = -5))) %>%\r\n  mutate(year_added = as.numeric(str_sub(date_added, start = -4)))\r\n\r\n# Creating a vector of MPA film ratings\r\nMPA_ratings <- c(\"G\", \"PG\", \"PG-13\", \"R\", \"NC-17\")\r\n\r\n# Creating a list of the top categories (\"listed in\") on Netflix\r\ntop_listings <- tt$netflix_titles %>%\r\n  separate_rows(listed_in, sep = \", \") %>%\r\n  count(listed_in, sort = TRUE) %>%\r\n  select(listed_in) %>%\r\n  head()\r\n# Counting the occurrences of words in the descriptions of these top categories\r\ntop_listing_words <- tt$netflix_titles %>%\r\n  separate_rows(listed_in, sep = \", \") %>%\r\n  filter(listed_in %in% top_listings$listed_in) %>%\r\n  select(listed_in, description) %>%\r\n  unnest_tokens(word, description) %>%\r\n  anti_join(stop_words, by = \"word\") %>%\r\n  count(listed_in, word, sort = TRUE)\r\n# Counting the total number of words in the descriptions of these top categories\r\ntotal_words <- top_listing_words %>%\r\n  group_by(listed_in) %>%\r\n  summarise(total = sum(n))\r\n# Adding word totals to individual word counts\r\ntop_listing_words <- left_join(top_listing_words, total_words, by = \"listed_in\")\r\n# Adding tf-idf to these word counts\r\ntop_listing_words <- top_listing_words %>%\r\n  bind_tf_idf(word, listed_in, n)\r\n\r\n\r\n\r\nFilms for younger audiences tend to be shorter\r\nFor this plot, I selected the films in the data set which have Motion Picture Association (MPA) ratings. This is the American film rating system, with categories as follows:\r\nG General Audiences\r\nPG Parental Guidance Suggested\r\nPG-13 Parents Strongly Cautioned\r\nR Restricted\r\nNC-17 Adults Only\r\nWhen the duration of these films are plotted according to their MPA rating, we can see that films aimed at older audiences tend to be longer than those aimed at younger viewers.\r\n\r\nShow code\r\n# Plotting distributions of film length according to MPA rating\r\nnetflix_movies %>%\r\n  filter(type == \"Movie\" & !is.na(rating)) %>%\r\n  filter(rating %in% MPA_ratings) %>%\r\n  mutate(rating = factor(rating, levels = rev(MPA_ratings))) %>%\r\n  ggplot(aes(x = rating, y = runtime, fill = rating)) +\r\n  geom_violin() +\r\n  geom_hline(yintercept = 90, linetype = 2) +\r\n  coord_flip() +\r\n  theme_classic() +\r\n  scale_fill_viridis_d() +\r\n  theme(legend.position = \"none\") +\r\n  labs(x = \"Film rating\", y = \"Film duration (minutes)\",\r\n       title = \"Films aimed at younger audiences tend to be shorter\",\r\n       subtitle = \"Film rating vs duration in minutes, dashed line at 90 minutes\")\r\n\r\n\r\n\r\n\r\nNetflix is adding fewer films for kids\r\nFor this plot, I selected MPA rated films again, and the year they were added to Netflix. Each column represents a single year, and each column is divided according to the percentage of films in each MPA rating added that year. From this plot, we can see that Netflix is adding fewer films for kids (G and PG).\r\n\r\nShow code\r\n# Summarising films added to Netflix annually by MPA rating\r\nnetflix_movies %>%\r\n  filter(rating %in% MPA_ratings & !is.na(year_added)) %>%\r\n  select(year_added, rating) %>%\r\n  group_by(year_added) %>%\r\n  count(rating) %>%\r\n  mutate(percentage = n / sum(n)) %>%\r\n  mutate(rating = factor(rating, levels = rev(MPA_ratings))) %>%\r\n  ggplot(aes(x = year_added, y = percentage, fill = rating)) +\r\n  geom_col() +\r\n  theme_classic() +\r\n  scale_fill_viridis_d() +\r\n  scale_x_continuous(breaks = seq(2008, 2021, by = 1)) +\r\n  scale_y_continuous(labels = scales::percent_format(scale = 100)) +\r\n  labs(y = \"Percentage of films added\", x = \"Year\", fill = \"MPA rating\",\r\n       title = \"MPA rated films added to Netflix each year\",\r\n       subtitle = \"Year vs Percentage of Motion Picture Association (MPA) rated films added\")\r\n\r\n\r\n\r\n\r\nSignificant words in category descriptions\r\nIn this section, significant words used to describe titles in the most common Netflix categories are plotted. This is done by…\r\ntaking the descriptions of films/shows listed in these common categories as a corpus\r\nsplitting that corpus into a document for each category\r\ncalculating tf-idf to find significant words used to describe titles in each category\r\nFrom this plot, we can see that documentaries are associated with the most unique words. “Action & Adventure” titles are associated with “guts”, “revenge” and “007”. “International TV Shows” are associated with “Spain’s”, “parallel” and “docuseries”.\r\nThis plot is confounded by Netflix titles having multiple categories. For example, a crime show may have “Crime TV Shows” and “TV Dramas” as its categories. This interferes with detecting significant words used in descriptions according to category. In this plot, “documentary” is the most significant word used in the “Documentary” category, but it is also a significant word in the “International Movies” category.\r\n\r\nShow code\r\n# Plotting significant words used in Netflix descriptions\r\ntop_listing_words %>%\r\n  slice_max(tf_idf, n = 20) %>%\r\n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = listed_in)) +\r\n  geom_col() +\r\n  theme_classic() +\r\n  labs(y = \"Word\", x = \"Term frequency-inverse document frequency (tf-idf)\",\r\n       fill = \"Netflix categories\",\r\n       title = \"Significant words in common Netflix category descriptions\",\r\n       subtitle = \"Words that appear often in these categories and not others\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-21-netflix-titles/netflix-titles_files/figure-html5/figure1-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-16-us-post-offices/",
    "title": "Post offices in the USA from 1772 to 2000",
    "description": "Graphs and analysis using the #TidyTuesday data set for week 16 of 2021\n  (13/4/2021): \"US post offices\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2021-04-16",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSetup\r\nPlotting US post offices from 1772 to 2000\r\n\r\nSetup\r\nLoading the R libraries, data set, and a shapefile for the USA.\r\n\r\nShow code\r\n# Loading libraries\r\nlibrary(tidyverse)\r\nlibrary(tidytuesdayR)\r\nlibrary(gganimate)\r\nlibrary(sf)\r\n\r\n# Loading data set\r\ntt <- tt_load(\"2021-04-13\")\r\n\r\n\r\n\r\n    Downloading file 1 of 1: `post_offices.csv`\r\nShow code\r\n# Loading USA shapefile\r\nusa_shapefile <- read_sf(\"~/TidyTuesday/data/States_shapefile-shp/\")\r\n\r\n\r\n\r\nWrangling data for visualisation.\r\n\r\nShow code\r\n# Creating a filtered, tidy tibble with one row per post office with coordinates\r\npost_office_est <- tt$post_offices %>%\r\n  filter(coordinates == TRUE) %>%\r\n  filter(!is.na(established)) %>%\r\n  filter(established >= 1772 & established <= 2021) %>%\r\n  filter(longitude <= 100) %>%\r\n  filter(!is.na(stamp_index) & stamp_index <= 9) %>%\r\n  select(established, latitude, longitude, stamp_index)\r\n  \r\npost_office_est$established <- post_office_est$established %>% as.integer()\r\n\r\n\r\n\r\nPlotting US post offices from 1772 to 2000\r\nIn this section, post offices in the USA are plotted in the order they were established. This animation begins in the 1772 and ends in 2000. Each point represents one post office, with the colour of each point corresponding to the scarcity of postmarks from that post office. The lighter the point, the rarer the postmark. The order in which post offices are established follows the colonisation of America, with post offices first appearing on the east coast before moving west.\r\n\r\nShow code\r\n# Creating an animation with one point per post office from 1772 to 2000\r\np <- ggplot(usa_shapefile) +\r\n  geom_sf() +\r\n  geom_point(aes(longitude, latitude, colour = stamp_index),\r\n             data = post_office_est) +\r\n  scale_fill_continuous(type = \"gradient\") +\r\n  transition_time(established, range = c(1772L, 2000L)) +\r\n  ease_aes(\"linear\") +\r\n  theme_void() +\r\n  theme(legend.position = \"none\") +\r\n  labs(title = \"Post offices established each year in the US\",\r\n       subtitle = \"Lighter points indicate rarer postmarks. Year: {frame_time}\")\r\n\r\n# Rendering the animation as a .gif\r\nanimate(p, nframes = 400, fps = 5, renderer = magick_renderer())\r\n\r\n\r\n\r\n\r\nHere are all these post offices in one static plot.\r\n\r\nShow code\r\n# All the post offices on the USA shapefile in a static plot\r\nggplot(usa_shapefile) +\r\n  geom_sf() +\r\n  geom_point(aes(longitude, latitude, colour = stamp_index),\r\n             data = post_office_est) +\r\n  scale_fill_continuous(type = \"gradient\") +\r\n  theme_void() +\r\n  theme(legend.position = \"none\") +\r\n  labs(title = \"Post offices in the US\",\r\n       subtitle = \"Lighter points indicate rarer postmarks\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-16-us-post-offices/us-post-offices_files/figure-html5/figure2-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 960
  },
  {
    "path": "posts/2021-04-07-global-deforestation/",
    "title": "Plotting deforestation and its causes",
    "description": "Graphs and analysis using the #TidyTuesday data set for week 15 of 2021\n  (6/4/2021): \"Global deforestation\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2021-04-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSetup\r\nPlotting the causes of deforestation in Brazil\r\nPlotting net change in forest area across different countries\r\n\r\nSetup\r\nLoading the R libraries and data set.\r\n\r\nShow code\r\n# Loading libraries\r\nlibrary(tidyverse)\r\nlibrary(tidytuesdayR)\r\n\r\n# Loading data\r\ntt <- tt_load(\"2021-04-06\")\r\n\r\n\r\n\r\n    Downloading file 1 of 5: `forest.csv`\r\n    Downloading file 2 of 5: `forest_area.csv`\r\n    Downloading file 3 of 5: `brazil_loss.csv`\r\n    Downloading file 4 of 5: `soybean_use.csv`\r\n    Downloading file 5 of 5: `vegetable_oil.csv`\r\n\r\nWrangling data for visualisation.\r\n\r\nShow code\r\n# Creating a function to tidy the tt$brazil_loss data set\r\n## The function \"tidy_brazil_loss\" takes a variable name as an argument.\r\n## It returns that variable in a tidy format, with one row per observation and\r\n## one column per variable\r\ntidy_brazil_loss <- function(variable){\r\n  tt$brazil_loss %>% \r\n    select(year, variable) %>% \r\n    mutate(cause = as.character(variable)) %>% \r\n    select(year, cause, variable) %>%\r\n    rename(\"loss\" = variable)\r\n}\r\n\r\n# Creating a list of the variables which will be run through the\r\n# tidy_brazil_loss function\r\n## The first three variables (entity, code and year) are skipped: entity and\r\n## code are irrelevant as all the observations are from Brazil (i.e. one\r\n## country) and the value for \"year\" will be included in each observation in the\r\n## tidy data set\r\nvariable_names <- colnames(tt$brazil_loss)[4:14]\r\n# Applying the tidy_loss_function to all the variable names using purr::map\r\ntidy_brazil_list <- map(variable_names, tidy_brazil_loss)\r\n# Binding the tidy version of each variable by row using purr::map_df\r\ntidy_brazil <- map_df(tidy_brazil_list, rbind)\r\n\r\n# Changing the \"cause\" variable in tidy_brazil to a factor variable\r\ntidy_brazil$cause <- as.factor(tidy_brazil$cause)\r\n# Printing the levels of tidy_brazil$cause\r\nlevels(tidy_brazil$cause)\r\n\r\n\r\n [1] \"commercial_crops\"               \r\n [2] \"fire\"                           \r\n [3] \"flooding_due_to_dams\"           \r\n [4] \"mining\"                         \r\n [5] \"natural_disturbances\"           \r\n [6] \"other_infrastructure\"           \r\n [7] \"pasture\"                        \r\n [8] \"roads\"                          \r\n [9] \"selective_logging\"              \r\n[10] \"small_scale_clearing\"           \r\n[11] \"tree_plantations_including_palm\"\r\nShow code\r\n# Setting more descriptive factor levels for tidy_brazil$cause\r\nlevels(tidy_brazil$cause) <- c(\"Commercial crops\",\r\n                               \"Fire loss\",\r\n                               \"Flooding due to dams\",\r\n                               \"Mining\",\r\n                               \"Natural disturbances\",\r\n                               \"Infrastructure (not roads)\",\r\n                               \"Pasture for livestock\",\r\n                               \"Roads\",\r\n                               \"Logging for lumber\",\r\n                               \"Small scale clearing\",\r\n                               \"Tree plantations\")\r\n\r\n# Printing the start of the tidied version of tt$brazil_loss\r\ntidy_brazil\r\n\r\n\r\n# A tibble: 143 x 3\r\n    year cause              loss\r\n   <dbl> <fct>             <dbl>\r\n 1  2001 Commercial crops 280000\r\n 2  2002 Commercial crops 415000\r\n 3  2003 Commercial crops 550000\r\n 4  2004 Commercial crops 747000\r\n 5  2005 Commercial crops 328000\r\n 6  2006 Commercial crops 188000\r\n 7  2007 Commercial crops  79000\r\n 8  2008 Commercial crops  52000\r\n 9  2009 Commercial crops  57000\r\n10  2010 Commercial crops 100000\r\n# … with 133 more rows\r\nShow code\r\n# Getting the six countries with the highest net forest conversion\r\nnet_forest_max <- tt$forest %>% \r\n  filter(entity != \"World\") %>%\r\n  group_by(entity) %>% \r\n  summarise(total = sum(net_forest_conversion)) %>%\r\n  slice_max(total, n = 6)\r\n\r\n# Getting the six countries with the lowest net forest conversion\r\nnet_forest_min <- tt$forest %>% \r\n  filter(entity != \"World\") %>%\r\n  group_by(entity) %>% \r\n  summarise(total = sum(net_forest_conversion)) %>%\r\n  slice_min(total, n = 6)\r\n\r\n# Binding the objects with the six highest and six lowest net forest conversions\r\nnet_forest_countries <- rbind(net_forest_max, net_forest_min)\r\n\r\n\r\n\r\nPlotting the causes of deforestation in Brazil\r\nIn this section, the causes of deforestation in Brazil are plotted, along with the observed forest loss in hectares associated with each cause. Each cause is faceted into its own section of the plot, with the y-axis changing according to the scale of deforestation. For perspective, a dashed line is added at 10,000 hectares of forest loss for each cause. From this plot, we can see that creating pasture for livestock lead to massive deforestation from 2001 until ~2005, and deforestation due to mining did not cross the 10,000 hectares threshold until 2013.\r\n\r\nShow code\r\n# Plotting the causes of deforestation in Brazil\r\ntidy_brazil %>%\r\n  ggplot(aes(year, loss, colour = cause)) +\r\n  geom_line(size = 0.8) +\r\n  geom_hline(yintercept = 10000, linetype = 2, size = 0.5) +\r\n  facet_wrap(~cause, scales = \"free\") +\r\n  theme_classic() +\r\n  guides(colour = FALSE) +\r\n  labs(y = \"Forest loss (hectares)\", x = \"Time\",\r\n       title = \"Forest loss due to different causes in Brazil\",\r\n       subtitle = \"Dashed line added at 10,000 hectares for perspective\")\r\n\r\n\r\n\r\n\r\nPlotting net change in forest area across different countries\r\nIn this section, net forest conversions in hectares over time are plotted for twelve countries. These countries had the highest and lowest net changes in forest area in the data set. Dashed lines are added to put the scale of forest gains and losses into perspective. For each country, a dashed green line is added at a net increase of 250,000 hectares of forest, and a dashed red line is added at a net decrease of 250,000 hectares.\r\n\r\nShow code\r\n# Plotting net change in forest area across different countries\r\ntt$forest %>%\r\n  filter(entity %in% net_forest_countries$entity) %>%\r\n  select(entity, year, net_forest_conversion) %>%\r\n  ggplot(aes(year, net_forest_conversion, colour = entity)) +\r\n  geom_line(size = 0.9) +\r\n  geom_hline(yintercept = 0) +\r\n  geom_hline(yintercept = 250000, linetype = 2, colour = \"green\", size = 0.4) +\r\n  geom_hline(yintercept = -250000, linetype = 2, colour = \"red\", size = 0.4) +\r\n  facet_wrap(~entity, scales = \"free\") +\r\n  theme_classic() +\r\n  guides(colour = FALSE) +\r\n  labs(y = \"Net forest conversion (hectares)\", x = \"Time\",\r\n       title = \"Countries with highest and lowest net changes in forest area\",\r\n       subtitle = \"Dashed lines at +250,000 hectares (green) and -250,000 hectares (red)\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-07-global-deforestation/global-deforestation_files/figure-html5/figure1-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 1536
  },
  {
    "path": "posts/2021-04-06-makeup-shades/",
    "title": "Plotting foundations according to shade",
    "description": "Graphs and analysis using the #TidyTuesday data set for week 14 of 2021\n  (30/3/2021): \"Makeup Shades\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2021-04-06",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSetup and data preparation\r\nPlotting foundations according to lightness\r\nPlotting distributions of foundation lightness\r\nPlotting keywords associated with foundations of different shades\r\n\r\nSetup and data preparation\r\nLoading the R libraries and data set.\r\n\r\nShow code\r\n# Loading libraries\r\nlibrary(tidyverse)\r\nlibrary(tidytuesdayR)\r\nlibrary(viridis)\r\nlibrary(tidytext)\r\nlibrary(forcats)\r\nlibrary(ggridges)\r\n\r\n# Loading data set\r\ntt <- tt_load(\"2021-03-30\")\r\n\r\n\r\n\r\n    Downloading file 1 of 5: `ulta.csv`\r\n    Downloading file 2 of 5: `sephora.csv`\r\n    Downloading file 3 of 5: `allShades.csv`\r\n    Downloading file 4 of 5: `allNumbers.csv`\r\n    Downloading file 5 of 5: `allCategories.csv`\r\n\r\nWrangling data for visualisation.\r\n\r\nShow code\r\n# Selecting the 14 brands with the most foundations in the data set as\r\n# \"top_brands\"\r\ntop_brands <- tt$allShades %>%\r\n  select(brand) %>%\r\n  count(brand) %>%\r\n  slice_max(order_by = n, n = 14)\r\n\r\n# Selecting foundation names broken into individual words and lightness values \r\n# rounded to the nearest significant digit as \"simplified_names\"\r\nsimplified_names <- tt$allShades %>%\r\n  mutate(rounded = signif(lightness, digits = 1)) %>%\r\n  filter(!is.na(name)) %>%\r\n  filter(rounded %in% c(0.2, 0.4, 0.6, 0.8, 1.0)) %>%\r\n  select(name, rounded) %>%\r\n  unnest_tokens(word, name) %>%\r\n  count(rounded, word, sort = T)\r\n\r\n# Counting the total number of words per rounded lightness value\r\ntotal_words <- simplified_names %>%\r\n  group_by(rounded) %>%\r\n  summarise(total = sum(n))\r\n\r\n# Added word count totals and tf-idf values to \"simplified_names\", and changing\r\n# \"rounded\" to a factor variable with informative levels\r\nsimplified_names <- left_join(simplified_names, total_words, by = \"rounded\")\r\nsimplified_names <- simplified_names %>%\r\n  bind_tf_idf(word, rounded, n)\r\nsimplified_names$rounded <- as.factor(simplified_names$rounded)\r\ntable(simplified_names$rounded)\r\n\r\n\r\n\r\n0.2 0.4 0.6 0.8   1 \r\n 28 148 221 217  50 \r\nShow code\r\nlevels(simplified_names$rounded) <- c(\"Lightness: 0.2, n = 28\",\r\n                                      \"Lightness: 0.4, n = 148\",\r\n                                      \"Lightness: 0.6, n = 221\",\r\n                                      \"Lightness: 0.8, n = 217\",\r\n                                      \"Lightness: 1.0, n = 50\")\r\nsimplified_names\r\n\r\n\r\n# A tibble: 664 x 7\r\n   rounded                 word       n total     tf   idf tf_idf\r\n   <fct>                   <chr>  <int> <int>  <dbl> <dbl>  <dbl>\r\n 1 Lightness: 0.8, n = 217 light    156  1512 0.103  0.511 0.0527\r\n 2 Lightness: 0.6, n = 221 medium   129  1401 0.0921 0.223 0.0205\r\n 3 Lightness: 0.6, n = 221 tan      118  1401 0.0842 0.511 0.0430\r\n 4 Lightness: 0.8, n = 217 ivory    113  1512 0.0747 0.223 0.0167\r\n 5 Lightness: 0.8, n = 217 beige    104  1512 0.0688 0.223 0.0153\r\n 6 Lightness: 0.4, n = 148 deep      99   748 0.132  0     0     \r\n 7 Lightness: 0.8, n = 217 fair      92  1512 0.0608 0.511 0.0311\r\n 8 Lightness: 0.6, n = 221 beige     90  1401 0.0642 0.223 0.0143\r\n 9 Lightness: 0.8, n = 217 warm      82  1512 0.0542 0.223 0.0121\r\n10 Lightness: 0.6, n = 221 warm      80  1401 0.0571 0.223 0.0127\r\n# … with 654 more rows\r\n\r\nPlotting foundations according to lightness\r\nIn this plot, each point represents a single foundation from the 14 most represented brands in the data set. The colour of each point corresponds to the dominant shade of each foundation. These points are arranged according to the lightness of each foundation.\r\n\r\nShow code\r\n# Plotting all the foundations from \"top_brands\" according to lightness\r\ntt$allShades %>%\r\n  filter(brand %in% top_brands$brand) %>%\r\n  ggplot(aes(lightness, brand, colour = hex)) +\r\n  geom_jitter() +\r\n  scale_colour_identity() +\r\n  xlim(0, 1) +\r\n  theme_classic() +\r\n  geom_vline(xintercept = 0.25, linetype = \"dashed\") +\r\n  geom_vline(xintercept = 0.50, linetype = \"dashed\") +\r\n  geom_vline(xintercept = 0.75, linetype = \"dashed\") +\r\n  labs(y = \"\", x = \"Lightness\",\r\n       title = \"Foundations from different brands plotted according to lightness\",\r\n       subtitle = \"Each point represents the dominant colour of each foundation\")\r\n\r\n\r\n\r\n\r\nPlotting distributions of foundation lightness\r\nIn this plot, the distributions of foundations from the brands in the previous graph are plotted according to lightness. Across all these brands, lighter shades are more represented than darker shades.\r\n\r\nShow code\r\n# Plotting the distribution of foundations from \"top_brands\" according to\r\n# lightness\r\ntt$allShades %>%\r\n  filter(brand %in% top_brands$brand) %>%\r\n  ggplot(aes(lightness, brand, fill = brand, group = brand)) +\r\n  geom_density_ridges_gradient() +\r\n  scale_fill_viridis(discrete = TRUE) +\r\n  xlim(0, 1) +\r\n  theme_ridges() +\r\n  geom_vline(xintercept = 0.25, linetype = \"dashed\") +\r\n  geom_vline(xintercept = 0.50, linetype = \"dashed\") +\r\n  geom_vline(xintercept = 0.75, linetype = \"dashed\") +\r\n  theme(legend.position = \"none\") +\r\n  labs(y = \"Brands\", x = \"Lightness\",\r\n       title = \"Foundation shade distributions\",\r\n       subtitle = \"Distribution of foundations from different brands according to lightness\")\r\n\r\n\r\n\r\n\r\nPlotting keywords associated with foundations of different shades\r\nIn this section, keywords associated with foundations of different shades are plotted. This is done by…\r\ntaking all the available foundation names as a corpus\r\nsplitting that corpus into different documents based on rounded lightness values\r\ncalculating tf-idf to find significant words used to describe foundations according to their shade\r\nFrom this plot, we can see that the darkest (“Lightness: 0.2”) and lightest (“Lightness: 0.8”) foundations are associated with more descriptive, unique keywords than the intermediate shades.\r\n\r\nShow code\r\nsimplified_names %>%\r\n  group_by(rounded) %>%\r\n  slice_max(n = 5, order_by = tf_idf) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = rounded)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~rounded, ncol = 2, scales = \"free\") +\r\n  theme_classic() +\r\n  labs(x = \"Term frequency-inverse document freqeuncy (tf-idf)\", y = \"Keywords\",\r\n       title = \"Keywords associated with foundations of different lightnesses\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-06-makeup-shades/makeup-shades_files/figure-html5/figure1-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 1536
  },
  {
    "path": "posts/2021-03-30-un-votes/",
    "title": "UN Votes: Plotting votes on United Nations resolutions",
    "description": "Graphs and analysis using the #TidyTuesday data set for week 13 of 2021\n  (23/3/2021): \"UN Votes\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2021-03-30",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSetup and data preparation\r\nPlotting the countries that have agreed or disagreed with the most US votes\r\nPlotting votes on UN resolutions on different issues\r\nPlotting keywords in UN resolution descriptions\r\nPlotting the percentage of UN votes on each issue in data set\r\n\r\nSetup and data preparation\r\nLoading the R libraries and data set.\r\n\r\nShow code\r\n# Loading libraries\r\nlibrary(forcats)\r\nlibrary(tidytext)\r\nlibrary(tidyverse)\r\nlibrary(tidytuesdayR)\r\n\r\n# Loading data\r\ntt <- tt_load(\"2021-03-23\")\r\n\r\n\r\n\r\n    Downloading file 1 of 3: `unvotes.csv`\r\n    Downloading file 2 of 3: `roll_calls.csv`\r\n    Downloading file 3 of 3: `issues.csv`\r\n\r\nWrangling data for visualisation.\r\n\r\nShow code\r\n# Extracting US votes on each UN resolution\r\nUS_votes <- tt$unvotes %>%\r\n  filter(country_code == \"US\") %>%\r\n  select(rcid, vote)\r\ncolnames(US_votes) <- c(\"rcid\", \"US_vote\")\r\n\r\n# Combining US votes and resolution issues with the \"roll_calls\" data set for\r\n# important votes\r\nimportant_votes <- tt$roll_calls %>%\r\n  filter(rcid %in% US_votes$rcid &\r\n           importantvote == 1)\r\nimportant_votes <- left_join(important_votes, tt$unvotes,\r\n                                   by = \"rcid\")\r\nimportant_votes <- left_join(important_votes, US_votes,\r\n                                   by = \"rcid\")\r\nimportant_votes <- important_votes %>%\r\n  filter(country != \"United States\") %>%\r\n  mutate(agree_with_US = vote == US_vote)\r\nimportant_votes$agree_with_US <- as.factor(important_votes$agree_with_US)\r\nlevels(important_votes$agree_with_US)\r\n\r\n\r\n[1] \"FALSE\" \"TRUE\" \r\nShow code\r\nlevels(important_votes$agree_with_US) <- c(\"Votes in disagreement with US\",\r\n                                           \"Votes in agreement with US\")\r\nimportant_votes <- left_join(important_votes,\r\n                                   tt$issues, by = \"rcid\")\r\nimportant_votes\r\n\r\n\r\n# A tibble: 88,144 x 16\r\n    rcid session importantvote date       unres  amend  para short    \r\n   <dbl>   <dbl>         <dbl> <date>     <chr>  <dbl> <dbl> <chr>    \r\n 1  2491      38             1 1983-10-04 R/38/3     0     0 KAMPUCHEA\r\n 2  2491      38             1 1983-10-04 R/38/3     0     0 KAMPUCHEA\r\n 3  2491      38             1 1983-10-04 R/38/3     0     0 KAMPUCHEA\r\n 4  2491      38             1 1983-10-04 R/38/3     0     0 KAMPUCHEA\r\n 5  2491      38             1 1983-10-04 R/38/3     0     0 KAMPUCHEA\r\n 6  2491      38             1 1983-10-04 R/38/3     0     0 KAMPUCHEA\r\n 7  2491      38             1 1983-10-04 R/38/3     0     0 KAMPUCHEA\r\n 8  2491      38             1 1983-10-04 R/38/3     0     0 KAMPUCHEA\r\n 9  2491      38             1 1983-10-04 R/38/3     0     0 KAMPUCHEA\r\n10  2491      38             1 1983-10-04 R/38/3     0     0 KAMPUCHEA\r\n# … with 88,134 more rows, and 8 more variables: descr <chr>,\r\n#   country <chr>, country_code <chr>, vote <chr>, US_vote <chr>,\r\n#   agree_with_US <fct>, short_name <chr>, issue <chr>\r\nShow code\r\n# Counting the number of votes for each country that coincide with the US vote\r\n# on each resolution\r\nagree_with_US_count <- important_votes %>%\r\n  group_by(agree_with_US) %>%\r\n  select(country, agree_with_US) %>%\r\n  count(agree_with_US, country, sort = TRUE)\r\nagree_with_US_count$country <- as.factor(agree_with_US_count$country)\r\n\r\n# Counting the number of votes that are the same/different as the US vote on\r\n# UN resolutions in different categories\r\nvotes_by_issue <- important_votes %>%\r\n  filter(!is.na(issue)) %>%\r\n  select(date, agree_with_US, issue) %>%\r\n  group_by(date, issue) %>%\r\n  count(agree_with_US)\r\n\r\n# Calculating term frequency-inverse document frequency (tf-idf) for the\r\n# descriptions of UN resolutions across different issues\r\nissue_descr_words <- important_votes %>% \r\n  filter(!is.na(issue)) %>% \r\n  select(issue, descr) %>% \r\n  unnest_tokens(word, descr) %>%\r\n  count(issue, word, sort = TRUE)\r\nissue_descr_words$issue <- as.factor(issue_descr_words$issue)\r\n# Removing \"stop words\" from resolution descriptions (e.g. \"the\", \"and\", \"of\")\r\nissue_descr_words <- issue_descr_words %>%\r\n  anti_join(stop_words, by = \"word\")\r\ntotal_words <- issue_descr_words %>%\r\n  group_by(issue) %>%\r\n  summarise(total = sum(n))\r\nissue_descr_words <- left_join(issue_descr_words, total_words, by = \"issue\")\r\nissue_descr_words <- issue_descr_words %>%\r\n  bind_tf_idf(word, issue, n)\r\n\r\n\r\n\r\nPlotting the countries that have agreed or disagreed with the most US votes\r\nOne of the variables in this data set is importantvote, which is used to record whether or not each United Nations (UN) resolution is classified as important by the United States (US) State Department. This is taken from its “Voting Practices in the United Nations” report. This variable is used to select UN resolutions important to the US. For these important resolutions, the vote of each country is compared with the US vote. The number of times each country votes with or against the US is counted; the countries that have agreed/disagreed on the most resolutions are plotted below.\r\n\r\nShow code\r\n# Plotting countries that have agreed/disagreed with the most US votes\r\nagree_with_US_count %>%\r\n  slice_head(n = 15) %>%\r\n  ggplot(aes(n, fct_reorder(country, n), fill = agree_with_US)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~agree_with_US, ncol = 2, scales = \"free\") +\r\n  theme_classic() + \r\n  labs(x = \"Votes on important UN resolutions\", y = \"Countries\",\r\n       title = \"Votes on important United Nations (UN) resolutions\",\r\n       subtitle = \"Resolution importance classified by the United States (US) State Department\")\r\n\r\n\r\n\r\n\r\nPlotting votes on UN resolutions on different issues\r\nThe comparison of each UN resolution vote with the US vote is also used in this plot. In this case, it is used to gauge the amount of consensus on each issue.\r\n\r\nShow code\r\n# Plotting votes on different categories of UN resolutions over time\r\nvotes_by_issue %>%\r\n  ggplot(aes(x = date, y = n, group = agree_with_US, colour = agree_with_US)) +\r\n  geom_line(size = 0.8) +\r\n  facet_wrap(~issue, scales = \"free\") +\r\n  theme_classic() +\r\n  theme(legend.position = \"top\") +\r\n  theme(legend.text = element_text(size = 12)) +\r\n  labs(x = \"Time\", y = \"UN resolution votes\",\r\n       title = \"Votes on UN resolutions over time by issue\",\r\n       subtitle = \"Agreement/disagreement with US vote used to gauge consensus on each issue\",\r\n       colour = NULL)\r\n\r\n\r\n\r\n\r\nPlotting keywords in UN resolution descriptions\r\nThe different issues covered by each UN resolution is taken directly from the data set (tt$issues). In this section, significant words used to describe UN resolutions on each issue are plotted. This is done by…\r\ntaking the UN resolution descriptions in tt$roll_calls$descrl as a corpus\r\nsplitting that corpus into a document for each issue in tt$issues\r\ncalculating tf-idf to find significant words used to describe UN resolutions on each issue\r\nThese keywords outline which UN resolution issues pertain to different countries. For example, resolutions on “Human rights” frequently use the words “situation” and “Iran”; resolutions on “Economic development” frequently use the words “embargo” and “Cuba”; resolutions on “Colonialism” frequently use the words “territories” and “Palestinian”.\r\n\r\nShow code\r\n# Plotting keywords in UN resolution descriptions\r\nissue_descr_words %>%\r\n  group_by(\"issue\") %>%\r\n  slice_max(tf_idf, n = 35) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~issue, ncol = 2, scales = \"free\") +\r\n  labs(x = \"tf-idf\", y = NULL) +\r\n  theme_classic() +\r\n  labs(x = \"Term frequency-inverse document freqeuncy (tf-idf)\",\r\n       title = \"Keywords in UN resolution descriptions across different categories\",\r\n       subtitle = \"tf-idf applied to resolution descriptions on different issues\")\r\n\r\n\r\n\r\n\r\nPlotting the percentage of UN votes on each issue in data set\r\nThis pie chart illustrates the percentage of UN votes on each issue in the data set.\r\n\r\nShow code\r\n# Plotting percentage of votes in each UN resolution category\r\nimportant_votes %>% \r\n  filter(!is.na(issue)) %>% \r\n  select(issue) %>% \r\n  group_by(issue) %>% \r\n  count(issue) %>%  \r\n  ungroup() %>%\r\n  mutate(perc = round(n/sum(n) * 100)) %>%\r\n  ggplot(aes(x = \"\", y = perc, fill = issue)) + \r\n  geom_bar(stat=\"identity\", width=1) +\r\n  coord_polar(\"y\", start=0) +\r\n  theme_void() +\r\n  geom_text(aes(label = paste(perc, \"%\", sep = \"\")), position = position_stack(vjust=0.5)) +\r\n  labs(title = \"Percentage of UN resolution votes on different issues\",\r\n       subtitle = \"Data set covers 6,202 UN resolutions from 1946 to 2019\",\r\n       fill = \"Issue\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-30-un-votes/un-votes_files/figure-html5/figure2-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 960
  },
  {
    "path": "posts/2021-03-23-video-games-and-sliced/",
    "title": "Video Games and Sliced",
    "description": "Graphs and analysis using the #TidyTuesday data set for week 12 of 2021\n  (16/3/2021): \"Video Games and Sliced\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2021-03-23",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSetup\r\nPlotting Peak vs. Average number of players using 200 observations\r\nCombining “year” and “month” into a new variable\r\nPlotting the monthly player gains and losses for three of the most popular games\r\nDiscussion\r\n\r\nSetup\r\nLoading the R libraries and data set.\r\n\r\nShow code\r\n# Loading libraries\r\nlibrary(tidyverse)\r\n\r\n# Reading in the raw data from GitHub (I would use \"tt_load\", but I hit an API\r\n# rate limt)\r\ngames <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-16/games.csv')\r\n\r\n\r\n\r\nPlotting Peak vs. Average number of players using 200 observations\r\nFor this plot, the top 200 observations for average number of players at the same time are selected using slice_max(order_by = avg, n = 200). The peak and average number of players for these observations are plotted on a scatter plot. The colour of the points indicates the game used for each observation. Models are fit to illustrate trends in the data; these trends follow the featured three games. The game “Cyberpunk 2077” was filtered out before creating this plot, as it occurred in only one of the 200 observations.\r\n\r\nShow code\r\ngames %>% \r\n  select(gamename, avg, peak) %>%\r\n  # Filtering out Cyberpunk 2077 as it has only a single observation\r\n  filter(gamename != \"Cyberpunk 2077\") %>%\r\n  slice_max(order_by = avg, n = 200) %>%\r\n  ggplot(aes(x = avg, y = peak, colour = gamename)) + \r\n  geom_point() + \r\n  geom_smooth(formula = y ~ x) + \r\n  theme_bw() +\r\n  theme(legend.position = \"bottom\") +\r\n  labs(title = \"Peak vs. Average number of players online simultaneously\",\r\n       subtitle = \"Top 200 observations for Average used\",\r\n       y = \"Highest number of players at the same time\",\r\n       x = \"Average number of players at the same time\",\r\n       colour = \"Game\")\r\n\r\n\r\n\r\n\r\nCombining “year” and “month” into a new variable\r\nCombining the year and month variables makes it easier to track when each observation was recorded. Creating this new year_month variable using the as.date() function from {lubridate} ensures that it will be interpreted as a date.\r\n\r\nShow code\r\n# Creating a \"year_month\" variable with the year and month of each observation\r\n# using the lubridate \"as_date()\" function, by pasting together...\r\ngames$year_month <- lubridate::as_date(paste(\r\n  # ...the \"year\" variable...\r\n  games$year,\r\n  # ...the number of each month, obtained by matching the month names to the\r\n  # \"month.name\" built-in constant...\r\n  match(games$month, month.name),\r\n  # ...the number \"1\", as a dummy \"day\" value...\r\n  1,\r\n  # ...separated by a \"-\".\r\n  sep = \"-\"))\r\n\r\n# Printing the start of the \"games\" object with the new variable...\r\ngames\r\n\r\n\r\n# A tibble: 83,631 x 8\r\n   gamename  year month    avg    gain   peak avg_peak_perc year_month\r\n   <chr>    <dbl> <chr>  <dbl>   <dbl>  <dbl> <chr>         <date>    \r\n 1 Counter…  2021 Febr… 7.41e5  -2196. 1.12e6 65.9567%      2021-02-01\r\n 2 Dota 2    2021 Febr… 4.05e5 -27840. 6.52e5 62.1275%      2021-02-01\r\n 3 PLAYERU…  2021 Febr… 1.99e5  -2290. 4.47e5 44.4707%      2021-02-01\r\n 4 Apex Le…  2021 Febr… 1.21e5  49216. 1.97e5 61.4752%      2021-02-01\r\n 5 Rust      2021 Febr… 1.18e5 -24375. 2.24e5 52.4988%      2021-02-01\r\n 6 Team Fo…  2021 Febr… 1.01e5  18083. 1.34e5 75.7603%      2021-02-01\r\n 7 Grand T…  2021 Febr… 9.06e4 -10603. 1.46e5 61.9017%      2021-02-01\r\n 8 Tom Cla…  2021 Febr… 7.24e4  -5335. 1.13e5 63.8645%      2021-02-01\r\n 9 Rocket …  2021 Febr… 5.37e4  -5726. 1.03e5 51.9419%      2021-02-01\r\n10 Path of…  2021 Febr… 4.69e4   -766. 9.05e4 51.8229%      2021-02-01\r\n# … with 83,621 more rows\r\n\r\nPlotting the monthly player gains and losses for three of the most popular games\r\nThis plot uses the year_month variable on the x-axis and the gain variable on the y-axis to illustrate month-to-month gains and losses in average players. The three games used share the majority of the highest avg (average number of simultaenous players) values in the data set. This graph is faceted for each game. To put these gains and losses into perspective, dashed lines are added at plus and minus 100,000 players in each facet.\r\n\r\nShow code\r\ngames %>% \r\n  # Selecting the variables\r\n  select(gamename, year_month, gain) %>%\r\n  # Filtering the data set for three of the most popular games\r\n  filter(gamename == \"Dota 2\" |\r\n           gamename == \"PLAYERUNKNOWN'S BATTLEGROUNDS\" |\r\n           gamename == \"Counter-Strike: Global Offensive\") %>%\r\n  ggplot(aes(year_month, gain, fill = gamename)) +\r\n  geom_col() +\r\n  theme_classic() +\r\n  theme(legend.position = \"none\") +\r\n  # Adding dashed lines to put facets into perspective\r\n  geom_hline(yintercept = 100000, linetype = \"dashed\") +\r\n  geom_hline(yintercept = -100000, linetype = \"dashed\") +\r\n  # Faceting the plot for each game\r\n  facet_wrap(~gamename, scales = \"free\") +\r\n  labs(\r\n    title = \"Gains/Losses in average number of players online for three games\",\r\n    subtitle = \"Dashed lines added at +/-100,000 players for each game\",\r\n    x = \"Time\",\r\n    y = \"Gains/Losses in average number of players\"\r\n  )\r\n\r\n\r\n\r\n\r\nDiscussion\r\nThe two plots in this post illustrate peak number of players online, average number of players online, and changes in that average for three games. Of these games, PLAYERUNKNOWN’S BATTLEGROUNDS (PUBG) has the highest values for peak and average players by far. However, these high points were not sustained, with dramatic losses in average number of players per month in 2018. By contrast, Dota 2 has maintained a relatively steady player base, without dramatic gains or losses. Counter-Strike: Global Offensive’s spike in popularity around April 2020 coincides with the introduction of lockdown measures.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-23-video-games-and-sliced/video-games-and-sliced_files/figure-html5/figure1-1.png",
    "last_modified": "2022-07-05T16:23:36+01:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 960
  },
  {
    "path": "posts/2021-03-21-bechdel-test/",
    "title": "Bechdel Test",
    "description": "Graphs using the #TidyTuesday data set for week 11 of 2021 (9/3/2021):\n\"Bechdel Test\"",
    "author": [
      {
        "name": "Ronan Harrington",
        "url": "https://github.com/rnnh/"
      }
    ],
    "date": "2021-03-21",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSetup\r\nIllustrating the change in Bechdel Test results over time\r\nPlotting the directors most likely to pass/fail the Bechdel Test\r\nPlotting the writers most likely to pass/fail the Bechdel Test\r\n\r\nSetup\r\nLoading the R libraries and data set.\r\n\r\nShow code\r\n# Loading libraries\r\nlibrary(gganimate)\r\nlibrary(tidytuesdayR)\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(forcats)\r\n\r\n# Loading the Bechdel Test data set\r\ntt <- tt_load(\"2021-03-09\")\r\n\r\n\r\n\r\n    Downloading file 1 of 2: `raw_bechdel.csv`\r\n    Downloading file 2 of 2: `movies.csv`\r\n\r\nIllustrating the change in Bechdel Test results over time\r\nThe first graph we want to create is an animation showing the change in Bechdel Test results over time. This animation shows the percentage of films each year (from 1940 to 2020) that meet different criteria of the Bechdel Test.\r\n\r\n\r\n# Changing \"rating\" from a character to a factor variable\r\ntt$raw_bechdel$rating <- as.factor(tt$raw_bechdel$rating)\r\n\r\n# Levels of the \"rating\" variable\r\nlevels(tt$raw_bechdel$rating)\r\n\r\n\r\n[1] \"0\" \"1\" \"2\" \"3\"\r\n\r\n# Renaming the levels of the \"rating\" variable\r\nlevels(tt$raw_bechdel$rating) <- c(\"Unscored\", \"It has two women...\",\r\n                                   \"...who talk to each other...\",\r\n                                   \"...about something besides a man\")\r\n\r\n# Counting the number of films with each Bechdel test rating per year\r\nratings_by_year <- tt$raw_bechdel %>%\r\n  group_by(year) %>%\r\n  count(year, rating)\r\n\r\n# Counting the total number of films in each year\r\nfilm_count_by_year <- ratings_by_year %>%\r\n  group_by(year) %>%\r\n  summarise(total = sum(n))\r\n\r\n# Adding the annual film count to the Bechdel test rating count per year\r\nratings_by_year <- left_join(ratings_by_year, film_count_by_year)\r\nrmarkdown::paged_table(ratings_by_year)\r\n\r\n\r\n\r\n\r\n\r\n# Changing \"year\" to an integer variable\r\nratings_by_year$year <- as.integer(ratings_by_year$year)\r\n\r\n# Creating an animation summarising the Bechdel test results from 1940 to 2020\r\np <- ratings_by_year %>%\r\n  ggplot(aes(x = fct_rev(rating), y = (n/total), group = rating, fill = rating)) +\r\n  geom_bar(stat = \"identity\") +\r\n  scale_y_continuous(labels = scales::percent_format(scale = 100)) +\r\n  coord_flip() +\r\n  theme_bw() +\r\n  theme(legend.position = \"none\") +\r\n  transition_time(year, range = c(1940L, 2020L)) +\r\n  labs(x = \"Bechdel Test result\", y = \"Percentage of films\",\r\n       subtitle = \"Year: {frame_time}\",\r\n       title = \"Bechdel Test results over time\") +\r\n  ease_aes(\"cubic-in-out\")\r\n\r\n# Rendering the animation as a .gif\r\nanimate(p, nframes = 400, fps = 20, renderer = magick_renderer())\r\n\r\n\r\n\r\n\r\nFigure 1: The percentage of films released each year with various Bechdel test results\r\n\r\n\r\n\r\nPlotting the directors most likely to pass/fail the Bechdel Test\r\nIn this section, a plot is produced that shows the directors most likely to pass/fail the Bechdel Test. This is done by…\r\ntaking all the directors in tt$movies as a corpus\r\nsplitting that corpus into two documents: Bechdel Test passes and failures\r\ncalculating tf-idf to find significant directors in each document\r\n\r\n\r\n# Selecting directors and their Bechdel pass/fail results\r\nresults_by_director <- tt$movies %>% \r\n  select(director, binary) %>% \r\n  filter(!is.na(director)) %>% \r\n  separate_rows(director, sep = \", \")\r\n\r\n# Changing \"binary\" to a factor variable\r\nresults_by_director$binary <- as.factor(results_by_director$binary)\r\n\r\n# Renaming the levels of the \"binary\" factor\r\nlevels(results_by_director$binary) <- c(\"Bechdel Test Failed\",\r\n                                        \"Bechdel Test Passed\")\r\n\r\n# Counting the number of times each director passes/fails the Bechdel test\r\nresults_by_director <- results_by_director %>% \r\n  count(binary, director, sort = TRUE)\r\n\r\n# Counting the number of times the Bechdel test has been passed/failed\r\ntotal_results <- results_by_director %>%\r\n  group_by(binary) %>%\r\n  summarise(total = sum(n))\r\n\r\n# Adding the total Bechdel result counts to \"results_by_director\"\r\nresults_by_director <- left_join(results_by_director, total_results)\r\n\r\n# Adding tf-idf values to \"results_by_director\"\r\nresults_by_director <- results_by_director %>% \r\n  bind_tf_idf(director, binary, n)\r\n\r\nrmarkdown::paged_table(results_by_director)\r\n\r\n\r\n\r\n\r\n\r\n# Plotting the directors with the highest tf-idf values based on Bechdel\r\n# test results\r\nresults_by_director %>%\r\n  group_by(binary) %>%\r\n  slice_max(tf_idf, n = 10) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(tf_idf, fct_reorder(director, tf-idf), fill = binary)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~binary, ncol = 2, scales = \"free\") +\r\n  labs(x = \"term frequency–inverse document frequency (tf-idf)\", y = NULL) +\r\n  labs(title = \"Directors most likely to Pass/Fail the Bechdel Test\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nFigure 2: Directors most likely to produce films that pass/fail the Bechdel test\r\n\r\n\r\n\r\nPlotting the writers most likely to pass/fail the Bechdel Test\r\nIn this section, the same process as the previous section is applied to the writers in tt$movies to find who is most likely to write for a film that passes/fails the Bechdel Test.\r\n\r\nShow code\r\n# Selecting writers and their Bechdel pass/fail results\r\nresults_by_writer <- tt$movies %>% \r\n  select(writer, binary) %>% \r\n  filter(!is.na(writer)) %>% \r\n  filter(!writer == \"N/A\") %>%\r\n  mutate(writer = str_remove(writer, \" \\\\(.*\")) %>%\r\n  separate_rows(writer, sep = \", \")\r\n\r\n# Changing \"binary\" to a factor variable\r\nresults_by_writer$binary <- as.factor(results_by_writer$binary)\r\n\r\n# Renaming the levels of the \"binary\" factor\r\nlevels(results_by_writer$binary) <- c(\"Bechdel Test Failed\",\r\n                                      \"Bechdel Test Passed\")\r\n\r\n# Counting the number of times each writer passes/fails the Bechdel test\r\nresults_by_writer <- results_by_writer %>% \r\n  count(binary, writer, sort = TRUE)\r\n\r\n# Counting the number of times the Bechdel test has been passed/failed\r\ntotal_results <- results_by_writer %>%\r\n  group_by(binary) %>%\r\n  summarise(total = sum(n))\r\n\r\n# Adding the total Bechdel result counts to \"results_by_writer\"\r\nresults_by_writer <- left_join(results_by_writer, total_results)\r\n\r\n# Adding tf-idf values to \"results_by_director\"\r\nresults_by_writer <- results_by_writer %>% \r\n  bind_tf_idf(writer, binary, n)\r\n\r\nrmarkdown::paged_table(results_by_writer)\r\n\r\n\r\n\r\n\r\nShow code\r\n# Plotting the directors with the highest tf-idf values based on Bechdel\r\n# test results\r\nresults_by_writer %>%\r\n  group_by(binary) %>%\r\n  slice_max(tf_idf, n = 10) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(tf_idf, fct_reorder(writer, tf-idf), fill = binary)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~binary, ncol = 2, scales = \"free\") +\r\n  labs(x = \"term frequency–inverse document frequency (tf-idf)\", y = NULL) +\r\n  labs(title = \"Writers most likely to Pass/Fail the Bechdel Test\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nFigure 3: Writers most likely to produce films that pass/fail the Bechdel test\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-21-bechdel-test/bechdel-test_files/figure-html5/figure1-1.gif",
    "last_modified": "2022-07-05T16:23:35+01:00",
    "input_file": {}
  }
]
